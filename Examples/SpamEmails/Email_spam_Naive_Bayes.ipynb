{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给定开源数据集enron1；\n",
    "\n",
    "## 查看下合法邮件和垃圾邮件的内容 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "file_path = 'enron1/ham/0007.1999-12-14.farmer.ham.txt'\n",
    "with open(file_path,'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "print(ham_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = 'enron1/spam/0058.2003-12-21.GP.spam.txt'\n",
    "with open(file_path,'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化变量：从最原始文档，到读取内容，到训练集labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "emails, labels =[], []\n",
    "file_path = 'enron1/spam/'\n",
    "for filename in glob.glob(os.path.join(file_path, '*.txt')):     # 遍历垃圾邮件的文档list\n",
    "    with open(filename, 'r', encoding = 'ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())                             # 读取邮件内容，并存入email list\n",
    "        labels.append(1)                                         # 垃圾邮件的label 指定为1，合法邮件为0\n",
    "\n",
    "# ham email:\n",
    "ham_file_path = 'enron1/ham/'\n",
    "for filename in glob.glob(os.path.join(ham_file_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding = 'ISO-8859-1') as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗原始文本数据：\n",
    "\n",
    "    * 清除数字和标点符号\n",
    "    * 移除人名（可选）\n",
    "    * 移除停用词\n",
    "    * 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def letters_only(astr):\n",
    "    return astr.isalpha()\n",
    "\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 将上述method 合为一个function 来做文本清洗：\n",
    "\n",
    "def clean_text(docs): # 参数输入为emails list\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                                     for word in doc.split()\n",
    "                                     if letters_only(word) and word not in all_names]))\n",
    "    return cleaned_docs\n",
    "\n",
    "cleaned_emails = clean_text(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移除停用词，提取特征，**向量化**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 481)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 285)\t1\n",
      "  (0, 424)\t1\n",
      "  (0, 250)\t1\n",
      "  (0, 345)\t1\n",
      "  (0, 445)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 497)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 178)\t2\n",
      "  (0, 125)\t2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=500) # 只考虑最频繁的500 terms；可调整获得更好的accuracy\n",
    "term_docs = cv.fit_transform(cleaned_emails) # 向量化，将document matrix（rows of words）转换为一个 term document matrix\n",
    "print(term_docs[0])                          # 每行是一个文档、一份email的词频稀疏向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    (row index, feature/term index) 词频      ——该稀疏向量的形式\n",
    "查看对应的term："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'website'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "feature_names[481]  # 上面第一个term\n",
    "feature_names[357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或者查看vocabulary dictionary：{feature: feature_index}\n",
    "feature_mapping=cv.vocabulary_\n",
    "#feature_mapping\n",
    "feature_mapping['read']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到term_docs后，就可以建模了，用Naive Bayes model\n",
    "\n",
    "分别计算prior, likelihood, evidence\n",
    "\n",
    "![naive bayes](nb_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据按label分组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_index(labels):\n",
    "    from collections import defaultdict # defaultdict: 果希望dict 的key不存在时，返回一个默认值\n",
    "    label_index = defaultdict(list)     # 相当于 label_index ={} 空字典，只不过下面引用key时，若key不存在，则返回值为列表\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)  # test example见下文\n",
    "    return label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [1, 2, 3]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'a': [1, 2, 3, 4]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(a=[1,2,3])\n",
    "d\n",
    "d['a'].append(4)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_index = get_label_index(labels) # 其实就是ham/spam : 0/1 2组\n",
    "# 训练样本的indices 按0/1类分组 {0: [3000, 3001, 3002, 3003, ...... 6670, 6671], 1: [0, 1, 2, 3, ...., 2998, 2999]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算prior："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prior(label_index):\n",
    "    \"\"\"Compute prior based on training samples\n",
    "    Args:\n",
    "        label_index (grouped sample indices by class)\n",
    "    Return:\n",
    "        dictionary, with class label as key, corresponding prior as the value\n",
    "    \"\"\"\n",
    "    prior = {label: len(index_list) for label, index_list in label_index.items()} # {\"0\":3672,\"1\":1500}\n",
    "    total_count = sum(prior.values())  # sum(dict_values([3672,1500]))\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7099767981438515, 1: 0.2900232018561485}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = get_prior(label_index)\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算likelihood："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    \"\"\"Compute likelihood based on training samples.\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        label_index (grouped sample indices by class)\n",
    "        smoothing (integer additive Laplace smoothing parameter)\n",
    "    Returns:\n",
    "        dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = get_likelihood(term_docs, label_index, smoothing=1)\n",
    "len(likelihood[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有了prior 和likelihood，就能计算test／new sample的后验概率：\n",
    "一个trick：避免连乘导致下溢，取log 变成连加再对结果取e值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    \"\"\"Computer posterior of testing samples, based on prior and likelihood\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        prior (dictionary, with class label as key, corresponding prior as the value)\n",
    "        likelihood (dictionary, with class label as key, corresponding conditional probability vector as value)\n",
    "    Return:\n",
    "        dictionary, with class label as key, corresponding posterior as value\n",
    "    \"\"\"\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        # = exp(log(prior * likelihood))\n",
    "        # = exp(log(prior) + log(likelihood))\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        # exp(-1000)/exp(-999) 会导致除零错误，但它= exp(0)/exp(1) = exp(-1)\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                # 如果一个log值极大，就设其为无穷大\n",
    "                posterior[label] = float('inf')\n",
    "        # normalize so that all sum up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测new sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 0.0045311245507072767, 0: 0.99546887544929274}, {1: 0.99963843948151876, 0: 0.00036156051848121361}]\n"
     ]
    }
   ],
   "source": [
    "emails_test = [\n",
    "    '''Subject: flat screens\n",
    "    hello ,\n",
    "    please call or contact regarding the other flat screens requested .\n",
    "    trisha tlapek - eb 3132 b\n",
    "    michael sergeev - eb 3132 a\n",
    "    also the sun blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .\n",
    "    thanks\n",
    "    kevin moore''',\n",
    "    '''Subject: having problems in bed ? we can help !\n",
    "    cialis allows men to enjoy a fully normal sex life without having to plan the sexual act .\n",
    "    if we let things terrify us , life will not be worth living .\n",
    "    brevity is the soul of lingerie .\n",
    "    suspicion always haunts the guilty mind .''',\n",
    "]\n",
    "\n",
    "cleaned_test = clean_text(emails_test)\n",
    "term_docs_test = cv.transform(cleaned_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个邮件是合法邮件，第二个邮件是垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估\n",
    "拆分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3465, 3465)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1707, 1707)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "len(X_train),len(Y_train)\n",
    "len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzhun/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:28: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing=1)\n",
    "\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on 1707 testing samples is: 92.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0.0\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] > 0.5:   # 可省略 if actual ==0\n",
    "        correct += 1\n",
    "print('The accuracy on {0} testing samples is: {1:.1f}%'.format(len(Y_test), correct/len(Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从头代码实现一个模型，是学习机器学习模型的最好方式。当然，我们也可以走捷径，调用sklearn :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
