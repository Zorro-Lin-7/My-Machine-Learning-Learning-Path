{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习工作流\n",
    "1. 数据准备\n",
    "2. 训练集生成\n",
    "3. 算法训练、评估、选择\n",
    "4. 部署和监控\n",
    "\n",
    "从开始的数据源，到最终的机器学习系统：![ML workflow](MLworkflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 完全理解项目目标。比较依赖domain knowledge 和经验。如之前的广告点击预测、股价预测。\n",
    "2. 收集尽可能多、全的数据\n",
    "3. 保证特征值的一致性，比如gender字段的Male、M\n",
    "4. 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing data imputation："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "data_origin = [[30, 100],\n",
    "               [20, 50],\n",
    "               [35, np.nan],\n",
    "               [25, 80],\n",
    "               [30, 70],\n",
    "               [40, 60]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均值填充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  30.  100.]\n",
      " [  20.   50.]\n",
      " [  35.   72.]\n",
      " [  25.   80.]\n",
      " [  30.   70.]\n",
      " [  40.   60.]]\n"
     ]
    }
   ],
   "source": [
    "imp_mean = Imputer(missing_values='NaN', strategy='mean')\n",
    "imp_mean.fit(data_origin) # fit data_origin\n",
    "data_mean_imp = imp_mean.transform(data_origin) # transform data_origin\n",
    "print(data_mean_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中位数填充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  30.  100.]\n",
      " [  20.   50.]\n",
      " [  35.   70.]\n",
      " [  25.   80.]\n",
      " [  30.   70.]\n",
      " [  40.   60.]]\n"
     ]
    }
   ],
   "source": [
    "imp_median = Imputer(missing_values='NaN', strategy = 'median')\n",
    "imp_median.fit(data_origin)\n",
    "data_median_imp = imp_median.transform(data_origin)\n",
    "print(data_median_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带缺失值的新样本进来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  72.]\n",
      " [ 30.  72.]\n",
      " [ 30.  70.]\n",
      " [ 30.  72.]]\n"
     ]
    }
   ],
   "source": [
    "new = [[20, np.nan],\n",
    "       [30, np.nan],\n",
    "       [np.nan, 70],\n",
    "       [np.nan, np.nan]]\n",
    "new_mean_imp = imp_mean.transform(new)\n",
    "print(new_mean_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 110\n",
      "Score with the data set with missing samples removed: 0.39\n",
      "Score with the data set with missing values replaced by mean: 0.42\n",
      "Score with the full data set: 0.44\n"
     ]
    }
   ],
   "source": [
    "# Effects of discarding missing values and imputation\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "X_full, y = dataset.data, dataset.target\n",
    "\n",
    "\n",
    "# Simulate a corrupted data set by adding 25% missing values\n",
    "m, n = X_full.shape\n",
    "m_missing = int(m * 0.25)\n",
    "print(m, m_missing)\n",
    "\n",
    "# Randomly select m_missing samples\n",
    "np.random.seed(42)\n",
    "missing_samples = np.array([True] * m_missing + [False] * (m - m_missing))\n",
    "np.random.shuffle(missing_samples)\n",
    "\n",
    "# For each missing sample, randomly select 1 out of n features\n",
    "missing_features = np.random.randint(low=0, high=n, size=m_missing)\n",
    "# Represent missing values by nan\n",
    "X_missing = X_full.copy()\n",
    "X_missing[np.where(missing_samples)[0], missing_features] = np.nan\n",
    "\n",
    "\n",
    "# Discard samples containing missing values\n",
    "X_rm_missing = X_missing[~missing_samples, :]\n",
    "y_rm_missing = y[~missing_samples]\n",
    "\n",
    "# Estimate R^2 on the data set with missing samples removed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regressor = RandomForestRegressor(random_state=42, max_depth=10, n_estimators=100)\n",
    "score_rm_missing = cross_val_score(regressor, X_rm_missing, y_rm_missing).mean()\n",
    "print('Score with the data set with missing samples removed: {0:.2f}'.format(score_rm_missing))\n",
    "\n",
    "\n",
    "# Imputation with mean value\n",
    "imp_mean = Imputer(missing_values='NaN', strategy='mean')\n",
    "X_mean_imp = imp_mean.fit_transform(X_missing)\n",
    "# Estimate R^2 on the data set with missing samples removed\n",
    "regressor = RandomForestRegressor(random_state=42, max_depth=10, n_estimators=100)\n",
    "score_mean_imp = cross_val_score(regressor, X_mean_imp, y).mean()\n",
    "print('Score with the data set with missing values replaced by mean: {0:.2f}'.format(score_mean_imp))\n",
    "\n",
    "\n",
    "# Estimate R^2 on the full data set\n",
    "regressor = RandomForestRegressor(random_state=42, max_depth=10, n_estimators=500)\n",
    "score_full = cross_val_score(regressor, X_full, y).mean()\n",
    "print('Score with the full data set: {0:.2f}'.format(score_full))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并不能保证填充策略总是比舍弃好，可以采用交叉验证来得到最佳策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.训练集的生成\n",
    "## 数据预处理和特征工程\n",
    "\n",
    "数据预处理包括：类别特征编码、特征缩放、特征选择、降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果特征是类别特征，是否需要编码看之后所用的算法。如朴素贝叶斯、树算法可以直接处理，其他算法通常需要编码才能处理。\n",
    "\n",
    "因为特征生成的output是算法训练的input，因此2个阶段需要整体看待，而不是分离对待。\n",
    "\n",
    "### 是否需要进行特征选择，如果是，怎么进行？基于L1正则的Logistic 和随机森林。特征选择的好处：\n",
    "    \n",
    "    * 减少预测模型的训练时间，因为冗余或不相关的特征被排除了；\n",
    "    * 减少过拟合的发生；\n",
    "    * 可能提高性能，因为预测模型用更重要的特征进行学习，这需要交叉验证，如下。\n",
    "    \n",
    "采用手写数字数据集，SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits \n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于原数据集的accuracy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score weith the original data set: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score # 交叉验证给出评分，默认3折\n",
    "classifier = SVC(gamma=0.005)\n",
    "score = cross_val_score(classifier, X, y).mean()  # 默认3折，返回3各评分，所以取mean\n",
    "print('Score weith the original data set: {0:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先用随机森林做特征选择，基于这些特征再用SVC，并估计accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with the data set of top 10 features: 0.87\n",
      "Score with the data set of top 15 features: 0.93\n",
      "Score with the data set of top 25 features: 0.94\n",
      "Score with the data set of top 35 features: 0.92\n",
      "Score with the data set of top 45 features: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, criterion='gini', n_jobs=-1)\n",
    "random_forest.fit(X, y)\n",
    "\n",
    "# 按特征重要程度排序\n",
    "feature_sorted = np.argsort(random_forest.feature_importances_) \n",
    "\n",
    "# Select different number of top features\n",
    "K = [10, 15, 25, 35, 45]\n",
    "for k in K:\n",
    "    top_K_features = feature_sorted[-k:] # 重要特征的索引\n",
    "    X_k_selected = X[:, top_K_features]  # 切片\n",
    "    # 分别估计不同的 k 下的accuracy\n",
    "    classifier = SVC(gamma=0.005)\n",
    "    score_K_features = cross_val_score(classifier, X_k_selected, y).mean()\n",
    "    print(\"Score with the data set of top {0} features: {1:.2f}\".format(k, score_K_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选最重要的25个特征能得到最高的accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 是否降维？如何降维？\n",
    "降维的好处与特征选择相似。下面用PCA降维，构建不同维数的新数据集，训练并估计各accuracy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with the data set of top 10 components: 0.95\n",
      "Score with the data set of top 15 components: 0.95\n",
      "Score with the data set of top 25 components: 0.91\n",
      "Score with the data set of top 35 components: 0.89\n",
      "Score with the data set of top 45 components: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Keep different number of top components\n",
    "N = [10, 15, 25, 35, 45]\n",
    "for n in N:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_n_kept = pca.fit_transform(X)\n",
    "    # 估计accuracy\n",
    "    classifier = SVC(gamma=0.005)\n",
    "    score_n_components = cross_val_score(classifier, X_n_kept, y).mean()\n",
    "    print('Score with the data set of top {0} components: {1:.2f}'.format(n,score_n_components))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维到10或15维，accuracy都能有0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征缩放\n",
    "股价预测案例中，SGD线性回归和SVR要求特征标准化-移除平均并缩放到单位方差。那么什么适合需要特征缩放？\n",
    "\n",
    "通常，朴素贝叶斯和树算法对特征量纲的不同并不敏感，因为它们在乎特征的独立性。Logistic 或线性回归一般也不受影响，**除非w的优化方式是SGD**。\n",
    "\n",
    "大部分情况下，一个算法涉及到样本距离 就需要 缩放／标准化 features，比如SVC、SVR。任何使用SGD的算法也必须要做特征缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有领域知识，再好不过；若无领域知识，如何生成新特征？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization:\n",
    "设定一个阈值，将数值特征转换为'二元'特征。例如，垃圾邮件检测，特征（term） 'prize' 可以生成新特征'whether prize occurs':词频大于1的则特征值为1，否则为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "X = [[4],[1],[3],[0]]\n",
    "binarizer = Binarizer(threshold=2.0)\n",
    "X_new = binarizer.fit_transform(X)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization:\n",
    "通过有限的值，将数值特征转换为类别特征。Binarization可以看着是Discretization的特例，比如年龄段特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction:\n",
    "包括相加、相乘、2个数值特征的任意运算、2个分类特征的联合约束。例如，\n",
    "    \n",
    "    ’每周访问量‘和’每周销量‘可以生成‘每次访问的销量；\n",
    "    ‘兴趣’和‘职业’——> ‘兴趣和职业‘，其值如’engineer interested in sports‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial transformation多项式变换：\n",
    "特征a、b，可以生成二次多项式特征a^2, ab, b^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   2.   4.   4.   8.  16.]\n",
      " [  1.   1.   3.   1.   3.   9.]\n",
      " [  1.   3.   2.   9.   6.   4.]\n",
      " [  1.   0.   3.   0.   0.   9.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = [[2, 4],\n",
    "     [1, 3],\n",
    "     [3, 2],\n",
    "     [0, 3]]\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_new = poly.fit_transform(X)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意，新特征中包括 1 ——bias or intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记录每个特征如何生成\n",
    "这貌似不重要，但我们常常忘记一个特征从何而来。模型训练失败，或者想要创建新特征来提升模型性能时，我们经常要回溯。为了删除无效的特征，添加有效特征，我们必须要理清what and how。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 算法训练、评估、选择\n",
    "拿到一个机器学习问题，一个问题通常是：解决它的最好算法是什么？NO free lunch.\n",
    "\n",
    "我们需要尝试多种方法和调优，才能找出最佳方案。\n",
    "\n",
    "## 从简单的开始\n",
    "由于一个算法有多个参数需要调整，穷举法非常耗时，且计算开销很大。NB、LR、SVM 3种算法可以择一作为出发点，具体选择方针如下：\n",
    "\n",
    "    * 训练集大小\n",
    "    * 数据集维度\n",
    "    * 数据是否线性可分\n",
    "    * 特征是否独立\n",
    "    * bias 和 variance 的tolerance 和tradeoff\n",
    "    * 是否有online learning需求\n",
    "\n",
    "### Naive Bayes:\n",
    "\n",
    "    一个很简单的算法。对于特征独立、相对小的训练集，表现很好。\n",
    "    至于大数据集，因为不管真实情况如何，此时也能够假设其特征独立，所以表现也不错。\n",
    "    因为计算简单，NB的训练通常比其他算法快，但这可能导致高偏差、低方差。\n",
    "### Logistic Regression\n",
    "\n",
    "    分类问题中，应用最广的算法。也是所有初试中的首选。\n",
    "    数据线性可分或近似可分时，表现很好。\n",
    "    即是非线性可分，或能转换为线性可分，再用LR。\n",
    "    通过SGD优化方式，LR对大数据集极具扩展性，因此解决大数据问题可以很高效，同时支持在线学习。\n",
    "    LR 是低偏差、高方差算法，但我们可以添加正则项防止过拟合，如L1、L2、及2者混合。\n",
    "### SVM\n",
    "    \n",
    "    线性可分或不可分都适用。\n",
    "    线性可分，采用线性核可比肩LR。\n",
    "    线性不可分，可用非线性核，如RBF。\n",
    "    对于高维数据集，SVM也能表现得很好，LR得让步于SVM，例如新闻分类有上万维。\n",
    "    只要选对kernel和参数，SVM能实现很高的accuracy，但计算开销非常大。\n",
    "    \n",
    "### 随机森林（决策树）\n",
    "    \n",
    "    不在乎数据的线性可分性。\n",
    "    类别特征无需编码，直接可用。\n",
    "    易用性和可解释性很好。\n",
    "    随机森林是决策树的加强，但过拟合风险也加强。\n",
    "    性能可比SVM，但调参比SVM和神经网络简单很多。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降低过拟合\n",
    "\n",
    "    * 交叉验证\n",
    "    * 正则化\n",
    "    * 尽可能简单。复杂模型包括树算法的深度过深；线性回归的高项次多项式转换；复杂kernel的SVM。\n",
    "    * 集成学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 诊断过拟合及欠拟合\n",
    "学习曲线常用于评估模型的bias 和 variance:\n",
    "理想状态：![ideal](learning_curve_ideal.png)\n",
    "训练评分接近所求，测试评分也逐渐接近\n",
    "\n",
    "过拟合： ![overfitting](learning_curve_overfitting.png)\n",
    "训练得很好，测试不好\n",
    "\n",
    "欠拟合：![underfitting](learning_curve_underfitting.png)\n",
    "训练不好，测试也不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4.部署和监控\n",
    "保存结果模型，并将它们部署到新的数据上，以及监视性能，定期更新预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存、加载、复用模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.load_diabetes()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "num_new = 30\n",
    "X_train = X[:-num_new,:]\n",
    "y_train = y[:-num_new]\n",
    "X_new = X[-num_new:]\n",
    "y_new = y[-num_new:]\n",
    "\n",
    "# Data pre-processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "import pickle\n",
    "# Save the scaler\n",
    "pickle.dump(scaler, open('scaler.p', 'wb'))\n",
    "\n",
    "X_scaled_train = scaler.transform(X_train)\n",
    "\n",
    "# Regression model training\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(C=20)\n",
    "regressor.fit(X_scaled_train, y_train)\n",
    "\n",
    "# Save the Regressor\n",
    "pickle.dump(regressor, open('regressor.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scaler = pickle.load(open('scaler.p','rb'))\n",
    "my_regressor = pickle.load(open('regressor.p','rb'))\n",
    "\n",
    "X_scaled_new = my_scaler.transform(X_new)\n",
    "predictions = my_regressor.predict(X_scaled_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 监控\n",
    "现在机器学习系统已上线运行，为了确保一切正常，我们需要定期检测：除了实时预测的情况，同时也要记录ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check on the model, R^2: 0.613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('Health check on the model, R^2: {0:.3f}'.format(r2_score(y_new, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定期更新模型：\n",
    "如果性能越来越差，数据的模式就会发生变化。我们可以更新模型。这取决于这个模型能否在线学习，该模型可以用新数据在线更新或重新训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
