{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'csv', 'file']\n",
      "['This is an example.']\n",
      "['This', 'is', 'an', 'example.']\n",
      "['I', 'have', 'a', 'cat!']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"example.csv\",'rt') as f:\n",
    "    reader = csv.reader(f, delimiter=',',quotechar='\"')\n",
    "    for line in reader:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an example sent.', 'The sentence splitter weill split on sent markers.', 'Ohh really !', '!']\n"
     ]
    }
   ],
   "source": [
    "inputstring = 'This is an example sent. The sentence splitter weill split on sent markers. Ohh really !!'\n",
    "from nltk.tokenize import sent_tokenize\n",
    "all_sent = sent_tokenize(inputstring)\n",
    "print(all_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"Hi Everyone!    hola gr8\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Everyone!', 'hola', 'gr8']\n"
     ]
    }
   ],
   "source": [
    "# simpliest tokenizer\n",
    "print(s.split())                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', 'hola', 'gr8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hi Everyone!    hola gr8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "\n",
    "regexp_tokenize(s, pattern='\\w+')\n",
    "regexp_tokenize(s, pattern='\\d+')\n",
    "\n",
    "wordpunct_tokenize(s)\n",
    "blankline_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'at'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "#from nltk.stem.Snowball import SnowballStemmer\n",
    "\n",
    "pst = PorterStemmer()   # create obj of the PorterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "pst.stem(\"eating\")\n",
    "pst.stem(\"shopping\")\n",
    "pst.stem('ate')\n",
    "\n",
    "lst.stem(\"eating\")\n",
    "lst.stem(\"shopping\")\n",
    "lst.stem(\"ate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ate'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize('ate')  # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'test']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "text = \"This is just a test\"\n",
    "cleanwordlist = [word for word in text.split() if word not in stoplist]\n",
    "\n",
    "cleanwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rare word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'apple': 2,\n",
       "          'bread': 1,\n",
       "          'cat': 4,\n",
       "          'dog': 1,\n",
       "          'egg': 2,\n",
       "          'milk': 1,\n",
       "          'orange': 1,\n",
       "          'tomato': 2})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = ['cat','cat','cat','cat','dog','apple','apple','bread','milk','egg','egg','orange','tomato','tomato']\n",
    "freq_dist = FreqDist(token)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 4), ('apple', 2), ('egg', 2), ('tomato', 2)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['cat', 'apple', 'egg', 'tomato']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rarewords = freq_dist.keys()[-2:]  报错 TypeError: 'dict_keys' object is not subscriptable\n",
    "\n",
    "# In Python 3 dict.keys() returns an iteratable but not indexable object.\n",
    "\n",
    "freq_dist.most_common(4)\n",
    "after_rare_words = [word for (word,freq) in freq_dist.most_common(4)]\n",
    "after_rare_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "edit_distance(\"rain\",\"train\")\n",
    "edit_distance(\"rain\",\"share\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
