{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rule-based approach:\n",
    "通过代码，人工设定语法规则："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context Free Grammer\n",
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toy_grammer = CFG.fromstring(\n",
    "\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "V -> 'eats' | 'drinks'\n",
    "NP -> DET N\n",
    "Det -> 'a' | 'an' | 'the'\n",
    "N -> 'president' | 'Obama' | 'apple' | 'coke'\n",
    "\"\"\")\n",
    "\n",
    "# 一个完整的句子 =                限定词 + 名词（词组） + 动词（词组） + 限定词 + 名词（词组）\n",
    "# S= NP + VP = NP + (V + NP) = (DET  +   N    ) +   (V + (        DET +    N    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP VP,\n",
       " VP -> V NP,\n",
       " V -> 'eats',\n",
       " V -> 'drinks',\n",
       " NP -> DET N,\n",
       " Det -> 'a',\n",
       " Det -> 'an',\n",
       " Det -> 'the',\n",
       " N -> 'president',\n",
       " N -> 'Obama',\n",
       " N -> 'apple',\n",
       " N -> 'coke']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_grammer.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以上给定的规则可生成的句子有：\n",
    "* President eats apple\n",
    "* Obama drinks coke\n",
    "\n",
    "但同样的语法也可能生成无意义的句子：\n",
    "* Apple eats coke\n",
    "* President drinks Obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of parsers (skim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests for the  CFG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import Nonterminal, nonterminals, Production, CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1 = Nonterminal(\"NP\")\n",
    "nt2 = Nonterminal(\"VP\")\n",
    "nt1.symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1 == Nonterminal('NP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt1 == nt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n",
    "N, V, P, DT = nonterminals('N, V, P, DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.symbol()\n",
    "VP.symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(DT, NP)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 = Production(S, [NP, VP])\n",
    "prod2 = Production(NP, [DT, NP])\n",
    "prod1.lhs() #Return the left-hand side of this ``Production``.\n",
    "prod2.rhs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 == Production(S, [NP, VP])\n",
    "prod1 == prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammer = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> 'the' N | PP | 'the' N PP\n",
    "    VP -> V NP | V PP | V NP PP\n",
    "    N -> 'cat' | 'dog' |'rug'\n",
    "    V -> 'chased'| 'sat'\n",
    "    P -> 'in'|'on'\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP VP,\n",
       " PP -> P NP,\n",
       " NP -> 'the' N,\n",
       " NP -> N PP,\n",
       " NP -> 'the' N PP,\n",
       " VP -> V NP,\n",
       " VP -> V PP,\n",
       " VP -> V NP PP,\n",
       " N -> 'cat',\n",
       " N -> 'dog',\n",
       " N -> 'rug',\n",
       " V -> 'chased',\n",
       " V -> 'sat',\n",
       " P -> 'in',\n",
       " P -> 'on']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Descent Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse import RecursiveDescentParser\n",
    "rd = RecursiveDescentParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分别解析歧义句和非歧义句\n",
    "sentence1 = 'the cat chased the dog'\n",
    "sentence2 = 'the cat chased the dog on the rug'\n",
    "tokens1 = sentence1.split()\n",
    "tokens2 = sentence2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the (N cat)) (VP (V chased) (NP the (N dog))))\n"
     ]
    }
   ],
   "source": [
    "for t in rd.parse(tokens1):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the (N cat))\n",
      "  (VP (V chased) (NP the (N dog) (PP (P on) (NP the (N rug))))))\n",
      "(S\n",
      "  (NP the (N cat))\n",
      "  (VP (V chased) (NP the (N dog)) (PP (P on) (NP the (N rug)))))\n"
     ]
    }
   ],
   "source": [
    "for t in rd.parse(tokens2):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sr (Shift Reduce Parser) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse import ShiftReduceParser\n",
    "sr = ShiftReduceParser(grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP the (N cat)) (VP (V chased) (NP the (N dog))))\n"
     ]
    }
   ],
   "source": [
    "for t in sr.parse(tokens1):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in sr.parse(tokens2):\n",
    "    print(t)     # 只会返回唯一的之前的解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当有多个可能的shift or reduce 操作可供选择时，sr 解析器采用heuristics 来做决策。而对于给定的语法，会选择错误的操作。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw a dog\n",
      "['I', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "|.    I    .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'I'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] NP -> * 'I'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'I' *\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "Nr edges in chart: 33\n",
      "(S (NP I) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, we test tracing with a short sentence\n",
    "nltk.parse.chart.demo(2, print_times=False, trace=1,sent='I saw a dog', numparses=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw John with a dog\n",
      "['I', 'saw', 'John', 'with', 'a', 'dog']\n",
      "\n",
      "* Strategy: Top-down\n",
      "\n",
      "Nr edges in chart: 48\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (Verb saw) (NP (NP John) (PP with (NP (Det a) (Noun dog))))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (VP (Verb saw) (NP John)) (PP with (NP (Det a) (Noun dog)))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# then we test the different parsing Strategies. Note that the number of edges differ between the strategies.\n",
    "# Top - Down\n",
    "nltk.parse.chart.demo(1,print_times=False, trace=0, sent='I saw John with a dog',numparses=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw John with a dog\n",
      "['I', 'saw', 'John', 'with', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "Nr edges in chart: 53\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (VP (Verb saw) (NP John)) (PP with (NP (Det a) (Noun dog)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP (Verb saw) (NP (NP John) (PP with (NP (Det a) (Noun dog))))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bottom - up\n",
    "nltk.parse.chart.demo(2,print_times=False, trace=0, sent='I saw John with a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexp parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk import regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_rules = regexp.ChunkRule('<.*>+','chunk everything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_parser = regexp.RegexpParser('''\n",
    " NP: {<DT>? <JJ>* <NN>*}\n",
    "  P: {<IN>}             \n",
    "  V: {<V.*>}     \n",
    " PP: {<P> <NP>}          \n",
    " VP: {<V> <NP|PP>*}  \n",
    "  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sent = \"Mr. Obama played a big role in the Health insurance bill\"\n",
    "tokens = nltk.word_tokenize(test_sent)\n",
    "test_sent_pos = nltk.pos_tag(tokens)\n",
    "paresed_out = reg_parser.parse(test_sent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Mr./NNP\n",
      "  Obama/NNP\n",
      "  (VP\n",
      "    (V played/VBD)\n",
      "    (NP a/DT big/JJ role/NN)\n",
      "    (PP (P in/IN) (NP the/DT)))\n",
      "  Health/NNP\n",
      "  (NP insurance/NN bill/NN))\n"
     ]
    }
   ],
   "source": [
    "print(paresed_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dependency parser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Parser [Very useful]\n",
    "\n",
    "http://nlp.stanford.edu:8080/parser/index.jsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser\\.jar jar file at stanford-parser.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-17b453225aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menglish_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanford-parser.jar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stanford-parser-3.4-models.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menglish_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this is the english parser test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at stanford-parser.jar"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "english_parser = StanfordParser('stanford-parser.jar','stanford-parser-3.4-models.jar')\n",
    "english_parser.raw_parse_sents((\"this is the english parser test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (skim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk.regexp import *\n",
    "test_sent = \"The prime minister announced he had asked the chief government whip, Philip Ruddock, to call\\\n",
    "            a special party room meeting for 9am on Monday to consider the spill motion.\"\n",
    "tokens = nltk.word_tokenize(test_sent)\n",
    "test_sent_pos = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  prime/JJ\n",
      "  minister/NN\n",
      "  (VP announced/VBD he/PRP)\n",
      "  (VP had/VBD asked/VBN)\n",
      "  the/DT\n",
      "  chief/JJ\n",
      "  government/NN\n",
      "  whip/NN\n",
      "  ,/,\n",
      "  Philip/NNP\n",
      "  Ruddock/NNP\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (VP call/VB)\n",
      "  a/DT\n",
      "  special/JJ\n",
      "  party/NN\n",
      "  room/NN\n",
      "  meeting/NN\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  to/TO\n",
      "  (VP consider/VB)\n",
      "  the/DT\n",
      "  spill/NN\n",
      "  motion/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "rule_vp = ChunkRule(r'(<VB.*>)?(<VB.*>)+(<PRP>)?','Chunk VPs')\n",
    "parser_vp = RegexpChunkParser([rule_vp], chunk_label='VP')\n",
    "print(parser_vp.parse(test_sent_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT prime/JJ minister/NN)\n",
      "  announced/VBD\n",
      "  he/PRP\n",
      "  had/VBD\n",
      "  asked/VBN\n",
      "  (NP the/DT chief/JJ government/NN whip/NN)\n",
      "  ,/,\n",
      "  (NP Philip/NNP Ruddock/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  call/VB\n",
      "  (NP a/DT special/JJ party/NN room/NN meeting/NN)\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  (NP Monday/NNP)\n",
      "  to/TO\n",
      "  consider/VB\n",
      "  (NP the/DT spill/NN motion/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "rule_np = ChunkRule(r'(<DT>?<RB>?)?<JJ|CD>*(<JJ|CD><,>)*(<NN.*>)+','Chunk NPs')\n",
    "parser_np = RegexpChunkParser([rule_np],chunk_label=\"NP\")\n",
    "print(parser_np.parse(test_sent_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "![Information extraction pipeline](pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tests showed that the chemical fipronil, which can harm people\\'s kidneys, liver and thyroid glands, was found in eggs from the Netherlands.\\nFipronil is used to treat lice and ticks in chickens.\\nOne German official said up to 10 million of the contaminated eggs may have been sold in Germany.\\nChristian Meyer, the agriculture minister for Lower Saxony, told German television that there was a risk to children if they ate two of the eggs a day.\\nAbout 180 poultry farms in the Netherlands have been temporarily shut in recent days while investigations are held.\\nMeanwhile, European supermarkets have moved to halt the distribution of eggs from the affected batches.\\nHowever, Aldi - which has close to 4,000 stores in Germany - is the first retailer to stop selling all eggs as a precaution.\\n\"This is merely a precaution, there is no reason to assume there are any health risks,\" Aldi said in a statement.\\nA spokeswoman for Aldi UK told the BBC its eggs were all British and were not affected by the contamination.\\nReuters reports that investigators believe the chemical may have originated in contaminated detergent used to clean barns.\\nPoultry World reported that fipronil may have been deliberately added to an existing insecticide to improve its effectiveness.\\nThe Netherlands is Europe\\'s largest exporter of eggs and egg products, and one of the biggest in the world. It exports an estimated 65% of the 10 billion eggs it produces every year.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('text_data.txt')\n",
    "text=f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Tests showed that the chemical fipronil, which can harm people's kidneys, liver and thyroid glands, was found in eggs from the Netherlands.\",\n",
       " 'Fipronil is used to treat lice and ticks in chickens.',\n",
       " 'One German official said up to 10 million of the contaminated eggs may have been sold in Germany.',\n",
       " 'Christian Meyer, the agriculture minister for Lower Saxony, told German television that there was a risk to children if they ate two of the eggs a day.',\n",
       " 'About 180 poultry farms in the Netherlands have been temporarily shut in recent days while investigations are held.',\n",
       " 'Meanwhile, European supermarkets have moved to halt the distribution of eggs from the affected batches.',\n",
       " 'However, Aldi - which has close to 4,000 stores in Germany - is the first retailer to stop selling all eggs as a precaution.',\n",
       " '\"This is merely a precaution, there is no reason to assume there are any health risks,\" Aldi said in a statement.',\n",
       " 'A spokeswoman for Aldi UK told the BBC its eggs were all British and were not affected by the contamination.',\n",
       " 'Reuters reports that investigators believe the chemical may have originated in contaminated detergent used to clean barns.',\n",
       " 'Poultry World reported that fipronil may have been deliberately added to an existing insecticide to improve its effectiveness.',\n",
       " \"The Netherlands is Europe's largest exporter of eggs and egg products, and one of the biggest in the world.\",\n",
       " 'It exports an estimated 65% of the 10 billion eggs it produces every year.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tests',\n",
       "  'showed',\n",
       "  'that',\n",
       "  'the',\n",
       "  'chemical',\n",
       "  'fipronil',\n",
       "  ',',\n",
       "  'which',\n",
       "  'can',\n",
       "  'harm',\n",
       "  'people',\n",
       "  \"'s\",\n",
       "  'kidneys',\n",
       "  ',',\n",
       "  'liver',\n",
       "  'and',\n",
       "  'thyroid',\n",
       "  'glands',\n",
       "  ',',\n",
       "  'was',\n",
       "  'found',\n",
       "  'in',\n",
       "  'eggs',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Netherlands',\n",
       "  '.'],\n",
       " ['Fipronil',\n",
       "  'is',\n",
       "  'used',\n",
       "  'to',\n",
       "  'treat',\n",
       "  'lice',\n",
       "  'and',\n",
       "  'ticks',\n",
       "  'in',\n",
       "  'chickens',\n",
       "  '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [ nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tokenized_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Tests', 'NNS'),\n",
       "  ('showed', 'VBD'),\n",
       "  ('that', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('chemical', 'NN'),\n",
       "  ('fipronil', 'NN'),\n",
       "  (',', ','),\n",
       "  ('which', 'WDT'),\n",
       "  ('can', 'MD'),\n",
       "  ('harm', 'VB'),\n",
       "  ('people', 'NNS'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('kidneys', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('liver', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('thyroid', 'JJ'),\n",
       "  ('glands', 'NNS'),\n",
       "  (',', ','),\n",
       "  ('was', 'VBD'),\n",
       "  ('found', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('eggs', 'NNS'),\n",
       "  ('from', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Netherlands', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('Fipronil', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('used', 'VBN'),\n",
       "  ('to', 'TO'),\n",
       "  ('treat', 'VB'),\n",
       "  ('lice', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('ticks', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('chickens', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "tagged_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Tests/NNS\n",
      "  showed/VBD\n",
      "  that/IN\n",
      "  the/DT\n",
      "  chemical/NN\n",
      "  fipronil/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  can/MD\n",
      "  harm/VB\n",
      "  people/NNS\n",
      "  's/POS\n",
      "  kidneys/NNS\n",
      "  ,/,\n",
      "  liver/NN\n",
      "  and/CC\n",
      "  thyroid/JJ\n",
      "  glands/NNS\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  found/VBN\n",
      "  in/IN\n",
      "  eggs/NNS\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (GPE Netherlands/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Fipronil/NNP)\n",
      "  is/VBZ\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  treat/VB\n",
      "  lice/NN\n",
      "  and/CC\n",
      "  ticks/NNS\n",
      "  in/IN\n",
      "  chickens/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  One/CD\n",
      "  (GPE German/JJ)\n",
      "  official/NN\n",
      "  said/VBD\n",
      "  up/RB\n",
      "  to/TO\n",
      "  10/CD\n",
      "  million/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  contaminated/VBN\n",
      "  eggs/NNS\n",
      "  may/MD\n",
      "  have/VB\n",
      "  been/VBN\n",
      "  sold/VBN\n",
      "  in/IN\n",
      "  (GPE Germany/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Christian/JJ)\n",
      "  (PERSON Meyer/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  agriculture/NN\n",
      "  minister/NN\n",
      "  for/IN\n",
      "  (PERSON Lower/NNP Saxony/NNP)\n",
      "  ,/,\n",
      "  told/VBD\n",
      "  (GPE German/JJ)\n",
      "  television/NN\n",
      "  that/IN\n",
      "  there/EX\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  risk/NN\n",
      "  to/TO\n",
      "  children/NNS\n",
      "  if/IN\n",
      "  they/PRP\n",
      "  ate/VBP\n",
      "  two/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  eggs/NNS\n",
      "  a/DT\n",
      "  day/NN\n",
      "  ./.)\n",
      "(S\n",
      "  About/IN\n",
      "  180/CD\n",
      "  poultry/NN\n",
      "  farms/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (GPE Netherlands/NNP)\n",
      "  have/VBP\n",
      "  been/VBN\n",
      "  temporarily/RB\n",
      "  shut/VBN\n",
      "  in/IN\n",
      "  recent/JJ\n",
      "  days/NNS\n",
      "  while/IN\n",
      "  investigations/NNS\n",
      "  are/VBP\n",
      "  held/VBN\n",
      "  ./.)\n",
      "(S\n",
      "  Meanwhile/RB\n",
      "  ,/,\n",
      "  (GPE European/JJ)\n",
      "  supermarkets/NNS\n",
      "  have/VBP\n",
      "  moved/VBN\n",
      "  to/TO\n",
      "  halt/VB\n",
      "  the/DT\n",
      "  distribution/NN\n",
      "  of/IN\n",
      "  eggs/NNS\n",
      "  from/IN\n",
      "  the/DT\n",
      "  affected/JJ\n",
      "  batches/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  However/RB\n",
      "  ,/,\n",
      "  (GPE Aldi/NNP)\n",
      "  -/:\n",
      "  which/WDT\n",
      "  has/VBZ\n",
      "  close/RB\n",
      "  to/TO\n",
      "  4,000/CD\n",
      "  stores/NNS\n",
      "  in/IN\n",
      "  (GPE Germany/NNP)\n",
      "  -/:\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  retailer/NN\n",
      "  to/TO\n",
      "  stop/VB\n",
      "  selling/VBG\n",
      "  all/DT\n",
      "  eggs/NNS\n",
      "  as/IN\n",
      "  a/DT\n",
      "  precaution/NN\n",
      "  ./.)\n",
      "(S\n",
      "  ``/``\n",
      "  This/DT\n",
      "  is/VBZ\n",
      "  merely/RB\n",
      "  a/DT\n",
      "  precaution/NN\n",
      "  ,/,\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  no/DT\n",
      "  reason/NN\n",
      "  to/TO\n",
      "  assume/VB\n",
      "  there/EX\n",
      "  are/VBP\n",
      "  any/DT\n",
      "  health/NN\n",
      "  risks/NNS\n",
      "  ,/,\n",
      "  ''/''\n",
      "  (PERSON Aldi/NNP)\n",
      "  said/VBD\n",
      "  in/IN\n",
      "  a/DT\n",
      "  statement/NN\n",
      "  ./.)\n",
      "(S\n",
      "  A/DT\n",
      "  spokeswoman/NN\n",
      "  for/IN\n",
      "  (PERSON Aldi/NNP UK/NNP)\n",
      "  told/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION BBC/NNP)\n",
      "  its/PRP$\n",
      "  eggs/NNS\n",
      "  were/VBD\n",
      "  all/DT\n",
      "  (GPE British/JJ)\n",
      "  and/CC\n",
      "  were/VBD\n",
      "  not/RB\n",
      "  affected/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  contamination/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Reuters/NNS\n",
      "  reports/NNS\n",
      "  that/IN\n",
      "  investigators/NNS\n",
      "  believe/VBP\n",
      "  the/DT\n",
      "  chemical/NN\n",
      "  may/MD\n",
      "  have/VB\n",
      "  originated/VBN\n",
      "  in/IN\n",
      "  contaminated/JJ\n",
      "  detergent/NN\n",
      "  used/VBN\n",
      "  to/TO\n",
      "  clean/JJ\n",
      "  barns/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Poultry/NN)\n",
      "  (PERSON World/NNP)\n",
      "  reported/VBD\n",
      "  that/IN\n",
      "  fipronil/NN\n",
      "  may/MD\n",
      "  have/VB\n",
      "  been/VBN\n",
      "  deliberately/RB\n",
      "  added/VBN\n",
      "  to/TO\n",
      "  an/DT\n",
      "  existing/VBG\n",
      "  insecticide/NN\n",
      "  to/TO\n",
      "  improve/VB\n",
      "  its/PRP$\n",
      "  effectiveness/NN\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  (GPE Netherlands/NNP)\n",
      "  is/VBZ\n",
      "  (GPE Europe/NNP)\n",
      "  's/POS\n",
      "  largest/JJS\n",
      "  exporter/NN\n",
      "  of/IN\n",
      "  eggs/NNS\n",
      "  and/CC\n",
      "  egg/NN\n",
      "  products/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  biggest/JJS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  world/NN\n",
      "  ./.)\n",
      "(S\n",
      "  It/PRP\n",
      "  exports/NNS\n",
      "  an/DT\n",
      "  estimated/VBN\n",
      "  65/CD\n",
      "  %/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  10/CD\n",
      "  billion/CD\n",
      "  eggs/NNS\n",
      "  it/PRP\n",
      "  produces/VBZ\n",
      "  every/DT\n",
      "  year/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sent in tagged_sentences:\n",
    "    print(nltk.ne_chunk(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "#组织与位置之间的关系已经被定义好了，我们要提取的是这些模式的所有组合\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')  # 这个pattern大致是提取 in 前后的名词\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN): # Organization and location\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
