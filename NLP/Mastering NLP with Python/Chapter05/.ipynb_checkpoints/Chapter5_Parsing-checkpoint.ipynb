{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treebank construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取文件indentifiers : fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BracketParseCorpusReader in '.../corpora/treebank/combined' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "print(str(nltk.corpus.treebank)) # corpus 所在路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.mrg',\n",
       " 'wsj_0002.mrg',\n",
       " 'wsj_0003.mrg',\n",
       " 'wsj_0004.mrg',\n",
       " 'wsj_0005.mrg',\n",
       " 'wsj_0006.mrg',\n",
       " 'wsj_0007.mrg',\n",
       " 'wsj_0008.mrg',\n",
       " 'wsj_0009.mrg',\n",
       " 'wsj_0010.mrg',\n",
       " 'wsj_0011.mrg',\n",
       " 'wsj_0012.mrg',\n",
       " 'wsj_0013.mrg',\n",
       " 'wsj_0014.mrg',\n",
       " 'wsj_0015.mrg',\n",
       " 'wsj_0016.mrg',\n",
       " 'wsj_0017.mrg',\n",
       " 'wsj_0018.mrg',\n",
       " 'wsj_0019.mrg',\n",
       " 'wsj_0020.mrg',\n",
       " 'wsj_0021.mrg',\n",
       " 'wsj_0022.mrg',\n",
       " 'wsj_0023.mrg',\n",
       " 'wsj_0024.mrg',\n",
       " 'wsj_0025.mrg',\n",
       " 'wsj_0026.mrg',\n",
       " 'wsj_0027.mrg',\n",
       " 'wsj_0028.mrg',\n",
       " 'wsj_0029.mrg',\n",
       " 'wsj_0030.mrg',\n",
       " 'wsj_0031.mrg',\n",
       " 'wsj_0032.mrg',\n",
       " 'wsj_0033.mrg',\n",
       " 'wsj_0034.mrg',\n",
       " 'wsj_0035.mrg',\n",
       " 'wsj_0036.mrg',\n",
       " 'wsj_0037.mrg',\n",
       " 'wsj_0038.mrg',\n",
       " 'wsj_0039.mrg',\n",
       " 'wsj_0040.mrg',\n",
       " 'wsj_0041.mrg',\n",
       " 'wsj_0042.mrg',\n",
       " 'wsj_0043.mrg',\n",
       " 'wsj_0044.mrg',\n",
       " 'wsj_0045.mrg',\n",
       " 'wsj_0046.mrg',\n",
       " 'wsj_0047.mrg',\n",
       " 'wsj_0048.mrg',\n",
       " 'wsj_0049.mrg',\n",
       " 'wsj_0050.mrg',\n",
       " 'wsj_0051.mrg',\n",
       " 'wsj_0052.mrg',\n",
       " 'wsj_0053.mrg',\n",
       " 'wsj_0054.mrg',\n",
       " 'wsj_0055.mrg',\n",
       " 'wsj_0056.mrg',\n",
       " 'wsj_0057.mrg',\n",
       " 'wsj_0058.mrg',\n",
       " 'wsj_0059.mrg',\n",
       " 'wsj_0060.mrg',\n",
       " 'wsj_0061.mrg',\n",
       " 'wsj_0062.mrg',\n",
       " 'wsj_0063.mrg',\n",
       " 'wsj_0064.mrg',\n",
       " 'wsj_0065.mrg',\n",
       " 'wsj_0066.mrg',\n",
       " 'wsj_0067.mrg',\n",
       " 'wsj_0068.mrg',\n",
       " 'wsj_0069.mrg',\n",
       " 'wsj_0070.mrg',\n",
       " 'wsj_0071.mrg',\n",
       " 'wsj_0072.mrg',\n",
       " 'wsj_0073.mrg',\n",
       " 'wsj_0074.mrg',\n",
       " 'wsj_0075.mrg',\n",
       " 'wsj_0076.mrg',\n",
       " 'wsj_0077.mrg',\n",
       " 'wsj_0078.mrg',\n",
       " 'wsj_0079.mrg',\n",
       " 'wsj_0080.mrg',\n",
       " 'wsj_0081.mrg',\n",
       " 'wsj_0082.mrg',\n",
       " 'wsj_0083.mrg',\n",
       " 'wsj_0084.mrg',\n",
       " 'wsj_0085.mrg',\n",
       " 'wsj_0086.mrg',\n",
       " 'wsj_0087.mrg',\n",
       " 'wsj_0088.mrg',\n",
       " 'wsj_0089.mrg',\n",
       " 'wsj_0090.mrg',\n",
       " 'wsj_0091.mrg',\n",
       " 'wsj_0092.mrg',\n",
       " 'wsj_0093.mrg',\n",
       " 'wsj_0094.mrg',\n",
       " 'wsj_0095.mrg',\n",
       " 'wsj_0096.mrg',\n",
       " 'wsj_0097.mrg',\n",
       " 'wsj_0098.mrg',\n",
       " 'wsj_0099.mrg',\n",
       " 'wsj_0100.mrg',\n",
       " 'wsj_0101.mrg',\n",
       " 'wsj_0102.mrg',\n",
       " 'wsj_0103.mrg',\n",
       " 'wsj_0104.mrg',\n",
       " 'wsj_0105.mrg',\n",
       " 'wsj_0106.mrg',\n",
       " 'wsj_0107.mrg',\n",
       " 'wsj_0108.mrg',\n",
       " 'wsj_0109.mrg',\n",
       " 'wsj_0110.mrg',\n",
       " 'wsj_0111.mrg',\n",
       " 'wsj_0112.mrg',\n",
       " 'wsj_0113.mrg',\n",
       " 'wsj_0114.mrg',\n",
       " 'wsj_0115.mrg',\n",
       " 'wsj_0116.mrg',\n",
       " 'wsj_0117.mrg',\n",
       " 'wsj_0118.mrg',\n",
       " 'wsj_0119.mrg',\n",
       " 'wsj_0120.mrg',\n",
       " 'wsj_0121.mrg',\n",
       " 'wsj_0122.mrg',\n",
       " 'wsj_0123.mrg',\n",
       " 'wsj_0124.mrg',\n",
       " 'wsj_0125.mrg',\n",
       " 'wsj_0126.mrg',\n",
       " 'wsj_0127.mrg',\n",
       " 'wsj_0128.mrg',\n",
       " 'wsj_0129.mrg',\n",
       " 'wsj_0130.mrg',\n",
       " 'wsj_0131.mrg',\n",
       " 'wsj_0132.mrg',\n",
       " 'wsj_0133.mrg',\n",
       " 'wsj_0134.mrg',\n",
       " 'wsj_0135.mrg',\n",
       " 'wsj_0136.mrg',\n",
       " 'wsj_0137.mrg',\n",
       " 'wsj_0138.mrg',\n",
       " 'wsj_0139.mrg',\n",
       " 'wsj_0140.mrg',\n",
       " 'wsj_0141.mrg',\n",
       " 'wsj_0142.mrg',\n",
       " 'wsj_0143.mrg',\n",
       " 'wsj_0144.mrg',\n",
       " 'wsj_0145.mrg',\n",
       " 'wsj_0146.mrg',\n",
       " 'wsj_0147.mrg',\n",
       " 'wsj_0148.mrg',\n",
       " 'wsj_0149.mrg',\n",
       " 'wsj_0150.mrg',\n",
       " 'wsj_0151.mrg',\n",
       " 'wsj_0152.mrg',\n",
       " 'wsj_0153.mrg',\n",
       " 'wsj_0154.mrg',\n",
       " 'wsj_0155.mrg',\n",
       " 'wsj_0156.mrg',\n",
       " 'wsj_0157.mrg',\n",
       " 'wsj_0158.mrg',\n",
       " 'wsj_0159.mrg',\n",
       " 'wsj_0160.mrg',\n",
       " 'wsj_0161.mrg',\n",
       " 'wsj_0162.mrg',\n",
       " 'wsj_0163.mrg',\n",
       " 'wsj_0164.mrg',\n",
       " 'wsj_0165.mrg',\n",
       " 'wsj_0166.mrg',\n",
       " 'wsj_0167.mrg',\n",
       " 'wsj_0168.mrg',\n",
       " 'wsj_0169.mrg',\n",
       " 'wsj_0170.mrg',\n",
       " 'wsj_0171.mrg',\n",
       " 'wsj_0172.mrg',\n",
       " 'wsj_0173.mrg',\n",
       " 'wsj_0174.mrg',\n",
       " 'wsj_0175.mrg',\n",
       " 'wsj_0176.mrg',\n",
       " 'wsj_0177.mrg',\n",
       " 'wsj_0178.mrg',\n",
       " 'wsj_0179.mrg',\n",
       " 'wsj_0180.mrg',\n",
       " 'wsj_0181.mrg',\n",
       " 'wsj_0182.mrg',\n",
       " 'wsj_0183.mrg',\n",
       " 'wsj_0184.mrg',\n",
       " 'wsj_0185.mrg',\n",
       " 'wsj_0186.mrg',\n",
       " 'wsj_0187.mrg',\n",
       " 'wsj_0188.mrg',\n",
       " 'wsj_0189.mrg',\n",
       " 'wsj_0190.mrg',\n",
       " 'wsj_0191.mrg',\n",
       " 'wsj_0192.mrg',\n",
       " 'wsj_0193.mrg',\n",
       " 'wsj_0194.mrg',\n",
       " 'wsj_0195.mrg',\n",
       " 'wsj_0196.mrg',\n",
       " 'wsj_0197.mrg',\n",
       " 'wsj_0198.mrg',\n",
       " 'wsj_0199.mrg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.treebank.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McDermott', 'International', 'Inc.', 'said', '0', ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(treebank.words('wsj_0007.mrg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('McDermott', 'NNP'), ('International', 'NNP'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(treebank.tagged_words('wsj_0007.mrg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Penn Treebank Corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Bailey) (NNP Controls))\n",
      "    (, ,)\n",
      "    (VP\n",
      "      (VBN based)\n",
      "      (NP (-NONE- *))\n",
      "      (PP-LOC-CLR\n",
      "        (IN in)\n",
      "        (NP (NP (NNP Wickliffe)) (, ,) (NP (NNP Ohio)))))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (VBZ makes)\n",
      "    (NP\n",
      "      (JJ computerized)\n",
      "      (JJ industrial)\n",
      "      (NNS controls)\n",
      "      (NNS systems)))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "print(treebank.parsed_sents('wsj_0007.mrg')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[0;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Vinken', 'NNP')]), ('is', 'VBZ'), Tree('NP', [('chairman', 'NN')]), ('of', 'IN'), Tree('NP', [('Elsevier', 'NNP'), ('N.V.', 'NNP')]), (',', ','), Tree('NP', [('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.chunked_sents()[1]\n",
    "#treebank_chunk.chunked_sents()[1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Elsevier', 'NNP'),\n",
       " ('N.V.', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Dutch', 'NNP'),\n",
       " ('publishing', 'VBG'),\n",
       " ('group', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.chunked_sents()[1].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Mr.', 'NNP'), 'NP'),\n",
       " (('Vinken', 'NNP'), 'NP'),\n",
       " (('is', 'VBZ'), 'S'),\n",
       " (('chairman', 'NN'), 'NP'),\n",
       " (('of', 'IN'), 'S'),\n",
       " (('Elsevier', 'NNP'), 'NP'),\n",
       " (('N.V.', 'NNP'), 'NP'),\n",
       " ((',', ','), 'S'),\n",
       " (('the', 'DT'), 'NP'),\n",
       " (('Dutch', 'NNP'), 'NP'),\n",
       " (('publishing', 'VBG'), 'NP'),\n",
       " (('group', 'NN'), 'NP'),\n",
       " (('.', '.'), 'S')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_chunk.chunked_sents()[1].pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/tokenize/regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[S -> NP ('is', 'VBZ') NP ('of', 'IN') NP (',', ',') NP ('.', '.'),\n",
       " NP -> ('Mr.', 'NNP') ('Vinken', 'NNP'),\n",
       " NP -> ('chairman', 'NN'),\n",
       " NP -> ('Elsevier', 'NNP') ('N.V.', 'NNP'),\n",
       " NP -> ('the', 'DT') ('Dutch', 'NNP') ('publishing', 'VBG') ('group', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_chunk.chunked_sents()[1].productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tags and frequency \n",
    "\n",
    "obtains a list of tags and the frequency of each tag in the Treebank corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import treebank\n",
    "fd = FreqDist()\n",
    "fd.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Sinica Treebank Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子'], ...]\n",
      "(S\n",
      "  (NP (NP (N‧的 (Nhaa 我) (DE 的)) (Ncb 腦海)) (Ncda 中))\n",
      "  (Dd 頓時)\n",
      "  (DM 一片)\n",
      "  (VH11 空白))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sinica_treebank\n",
    "print(sinica_treebank.sents())\n",
    "print(sinica_treebank.parsed_sents()[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting Context Free Grammar (CFG) rules from Treebank\n",
    "\n",
    "* A set of non terminal nodes (N)\n",
    "* A set of terminal nodes (T)\n",
    "* Start symbol (S)\n",
    "* A set of production rules (P) of the form:\n",
    "   \n",
    "## CFG rules are of two types—Phrase structure rules and Sentence structure rules:\n",
    "\n",
    "     A Phrase Structure Rule can be defined as follows A->a, where A and a consists of Terminals and Non terminals.\n",
    "    \n",
    "     In Sentence level Construction of CFG, there are four structures:\n",
    "        • Declarative structure: Deals with declarative sentences (the subject is followed by a predicate).\n",
    "        • Imperative structure: Deals with imperative sentences, commands,\n",
    "            or suggestions (sentences begin with a verb phrase and do not include a subject).\n",
    "        • Yes-No structure: Deals with question-answering sentences. The answers to these questions are either yes \n",
    "        or no.\n",
    "        • Wh-question structure: Deals with question-answering sentences. Questions that begin following Wh words               (Who, What, How, When, Where, Why, and Which).\n",
    "        \n",
    "General CFG rules are summarized here:\n",
    "![](CFG_rules.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:the use of Context-free Grammar rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'PP'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import Nonterminal, nonterminals, Production, CFG\n",
    "\n",
    "nonterminal1 = Nonterminal('NP')\n",
    "nonterminal2 = Nonterminal('VP')\n",
    "nonterminal3 = Nonterminal('PP')\n",
    "\n",
    "nonterminal1.symbol()\n",
    "nonterminal2.symbol()\n",
    "nonterminal3.symbol()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(NP, VP)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "VP"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(V, NP, NP, PP)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n",
    "N, V, P, DT = nonterminals('N, V, P, DT')\n",
    "\n",
    "production1 = Production(S, [NP, VP])\n",
    "production2 = Production(NP, [DT, NP])\n",
    "production3 = Production(VP, [V, NP, NP, PP])\n",
    "\n",
    "production1.lhs()\n",
    "production1.rhs()\n",
    "\n",
    "production3.lhs()\n",
    "production3.rhs()\n",
    "\n",
    "production3 == Production(VP, [V,NP,NP,PP])\n",
    "production2 == production3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example for accessing ATIS grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 5517 productions>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "gram1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the testing sentences from ATIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['list',\n",
       " 'those',\n",
       " 'flights',\n",
       " 'that',\n",
       " 'stop',\n",
       " 'over',\n",
       " 'in',\n",
       " 'salt',\n",
       " 'lake',\n",
       " 'city',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "len(sent)\n",
    "testingsent = sent[25]\n",
    "testingsent[1]\n",
    "testingsent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom-up parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13454\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser1 = nltk.parse.BottomUpChartParser(gram1)\n",
    "chart1 = parser1.chart_parse(sent)\n",
    "print((chart1.num_edges()))\n",
    "print((len(list(chart1.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom-up, Left Corner parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25] \n",
    "sent = testingsent[0]\n",
    "parser2 = nltk.parse.BottomUpLeftCornerChartParser(gram1)\n",
    "chart2 = parser2.chart_parse(sent)\n",
    "print((chart2.num_edges()))\n",
    "print((len(list(chart2.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Corner parsing with a Bottom-up  filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testing = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser3 = nltk.parse.LeftCornerChartParser(gram1)\n",
    "chart3 = parser3.chart_parse(sent)\n",
    "print((chart3.num_edges()))\n",
    "print((len(list(chart3.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-down parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testing[0]\n",
    "parser4 = nltk.parse.TopDownChartParser(gram1)\n",
    "chart4 = parser4.chart_parse(sent)\n",
    "print((chart4.num_edges()))\n",
    "print((len(list(chart4.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Incremental Bottom-up parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13454\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser5 = nltk.parse.IncrementalBottomUpChartParser(gram1)\n",
    "chart5 = parser5.chart_parse(sent)\n",
    "print((chart5.num_edges()))\n",
    "print(len(list(chart5.parses(gram1.start()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Incremental Bottom-up, Left Corner parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser6 = nltk.parse.IncrementalBottomUpLeftCornerChartParser(gram1)\n",
    "chart6 = parser6.chart_parse(sent)\n",
    "print((chart6.num_edges()))\n",
    "print((len(list(chart6.parses(gram1.start())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Incremental Left Corner parsing with a Bottom-up  filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent = sent[25]\n",
    "sent = testingsent[0]\n",
    "parser7 = nltk.parse.IncrementalLeftCornerChartParser(gram1)\n",
    "chart7 = parser7.chart_parse(sent)\n",
    "print((chart7.num_edges()))\n",
    "print(len(list(chart7.parses(gram1.start()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Incremental Top-down parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "sent = sent[25][0]\n",
    "parser8 = nltk.parse.IncrementalTopDownChartParser(gram1)\n",
    "chart8 = parser8.chart_parse(sent)\n",
    "print((chart8.num_edges()))\n",
    "print(len(list(chart8.parses(gram1.start()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Earley parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37763\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "sent = nltk.data.load('grammars/large_grammars/atis_sentences.txt')\n",
    "sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "testingsent=sent[25]\n",
    "sent=testingsent[0]\n",
    "parser9 = nltk.parse.EarleyChartParser(gram1)\n",
    "chart9 = parser9.chart_parse(sent)\n",
    "print((chart9.num_edges()))\n",
    "print(len(list(chart9.parses(gram1.start()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a probabilistic Context Free Grammar from CFG\n",
    "\n",
    "概率上下文无关文法（PCFG），CFG中的所有production rules 都附带概率。这些概率的总和是1。它生成与CFG相同的解析结构，但它也给每个解析树分配一个概率。通过生成树中所使用的所有生产规则的概率乘积，得到经过分析的树的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates the formation of rules in PCFG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from itertools import islice\n",
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram2 = PCFG.fromstring(\"\"\"\n",
    "   A -> B B [.3] | C B C [.7]\n",
    "   B -> B D [.5] | C [.5]\n",
    "   C -> 'a' [.1] | 'b' [0.9]\n",
    "   D -> 'b' [1.0]\n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A -> B B [0.3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod1 = gram2.productions()[0]\n",
    "prod1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A -> C B C [0.7]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod2 = gram2.productions()[1]\n",
    "prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(C, B, C)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod2.lhs()\n",
    "prod2.rhs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[A -> B B [0.3],\n",
       " A -> C B C [0.7],\n",
       " B -> B D [0.5],\n",
       " B -> C [0.5],\n",
       " C -> 'a' [0.1],\n",
       " C -> 'b' [0.9],\n",
       " D -> 'b' [1.0]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prod2.prob())\n",
    "gram2.start()\n",
    "gram2.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates Probabilistic Chart Parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 23 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> V NP [0.59]\n",
      "    VP -> V [0.4]\n",
      "    VP -> VP PP [0.01]\n",
      "    NP -> Det N [0.41]\n",
      "    NP -> Name [0.28]\n",
      "    NP -> NP PP [0.31]\n",
      "    PP -> P NP [1.0]\n",
      "    V -> 'saw' [0.21]\n",
      "    V -> 'ate' [0.51]\n",
      "    V -> 'ran' [0.28]\n",
      "    N -> 'boy' [0.11]\n",
      "    N -> 'cookie' [0.12]\n",
      "    N -> 'table' [0.13]\n",
      "    N -> 'telescope' [0.14]\n",
      "    N -> 'hill' [0.5]\n",
      "    Name -> 'Jack' [0.52]\n",
      "    Name -> 'Bob' [0.48]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "    Det -> 'the' [0.41]\n",
      "    Det -> 'a' [0.31]\n",
      "    Det -> 'my' [0.28]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from itertools import islice\n",
    "from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "tokens = 'Jack told Bob to bring my cookie'.split()\n",
    "grammar = toy_pcfg2\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYK chart parsing algorithm\n",
    "\n",
    "递归下降分析的缺点是它导致了左递归问题，而且非常复杂。因此，引入CYK图解析。它使用the Dynamic Programming approach\n",
    "CYK算法可以在O（n3）时间里构建一个chart\n",
    "\n",
    "Both CYK and Earley are Bottom-up chart parsing algorithms. But, the Earley algorithm also makes use of Top-down predictions when invalid parses are constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earley chart parsing algorithm\n",
    "\n",
    "该算法是类似于自顶向下分析。它可以处理左递归，而且不需要CNF。它填充表格的方式是“从左到右”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Earley chart parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "I saw a dog\n",
      "['I', 'saw', 'a', 'dog']\n",
      "\n",
      "|.    I    .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'I'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|>         .         .         .         .| [0:0] NP -> * Det Noun\n",
      "|>         .         .         .         .| [0:0] NP -> * 'I'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'I' *\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not all parses found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-00b7dd48953e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearleychart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'I saw a dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumparses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/parse/earleychart.py\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(print_times, print_grammar, print_trees, trace, sent, numparses)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;31m# Print results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnumparses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnumparses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not all parses found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_trees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not all parses found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.parse.earleychart.demo(print_times=False, trace=1, sent='I saw a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Chart parser in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "John saw a dog\n",
      "['John', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Bottom-up\n",
      "\n",
      "|.   John  .   saw   .    a    .   dog   .|\n",
      "|[---------]         .         .         .| [0:1] 'John'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] NP -> * 'John'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'John' *\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "Nr edges in chart: 33\n",
      "(S (NP John) (VP (Verb saw) (NP (Det a) (Noun dog))))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.parse.chart.demo(2, print_times=False, trace=1,sent='John saw a dog', numparses=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Stepping Chart parser in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sentence:\n",
      "John saw a dog\n",
      "['John', 'saw', 'a', 'dog']\n",
      "\n",
      "* Strategy: Stepping (top-down vs bottom-up)\n",
      "\n",
      "*** SWITCH TO TOP DOWN\n",
      "|[---------]         .         .         .| [0:1] 'John'\n",
      "|.         [---------]         .         .| [1:2] 'saw'\n",
      "|.         .         [---------]         .| [2:3] 'a'\n",
      "|.         .         .         [---------]| [3:4] 'dog'\n",
      "|>         .         .         .         .| [0:0] S  -> * NP VP\n",
      "|>         .         .         .         .| [0:0] NP -> * NP PP\n",
      "|>         .         .         .         .| [0:0] NP -> * Det Noun\n",
      "|>         .         .         .         .| [0:0] NP -> * 'John'\n",
      "|[---------]         .         .         .| [0:1] NP -> 'John' *\n",
      "|[--------->         .         .         .| [0:1] S  -> NP * VP\n",
      "|[--------->         .         .         .| [0:1] NP -> NP * PP\n",
      "|.         >         .         .         .| [1:1] VP -> * VP PP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb NP\n",
      "|.         >         .         .         .| [1:1] VP -> * Verb\n",
      "|.         >         .         .         .| [1:1] Verb -> * 'saw'\n",
      "|.         [---------]         .         .| [1:2] Verb -> 'saw' *\n",
      "|.         [--------->         .         .| [1:2] VP -> Verb * NP\n",
      "|.         [---------]         .         .| [1:2] VP -> Verb *\n",
      "|[-------------------]         .         .| [0:2] S  -> NP VP *\n",
      "|.         [--------->         .         .| [1:2] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] NP -> * NP PP\n",
      "|.         .         >         .         .| [2:2] NP -> * Det Noun\n",
      "*** SWITCH TO BOTTOM UP\n",
      "|.         .         >         .         .| [2:2] Det -> * 'a'\n",
      "|.         .         .         >         .| [3:3] Noun -> * 'dog'\n",
      "|.         .         [---------]         .| [2:3] Det -> 'a' *\n",
      "|.         .         .         [---------]| [3:4] Noun -> 'dog' *\n",
      "|.         .         [--------->         .| [2:3] NP -> Det * Noun\n",
      "|.         .         [-------------------]| [2:4] NP -> Det Noun *\n",
      "|.         [-----------------------------]| [1:4] VP -> Verb NP *\n",
      "|.         .         [------------------->| [2:4] NP -> NP * PP\n",
      "|[=======================================]| [0:4] S  -> NP VP *\n",
      "|.         [----------------------------->| [1:4] VP -> VP * PP\n",
      "|.         .         >         .         .| [2:2] S  -> * NP VP\n",
      "|.         .         [------------------->| [2:4] S  -> NP * VP\n",
      "*** SWITCH TO TOP DOWN\n",
      "|.         .         .         .         >| [4:4] VP -> * VP PP\n",
      "|.         .         .         .         >| [4:4] VP -> * Verb NP\n",
      "|.         .         .         .         >| [4:4] VP -> * Verb\n",
      "*** SWITCH TO BOTTOM UP\n",
      "*** SWITCH TO TOP DOWN\n",
      "*** SWITCH TO BOTTOM UP\n",
      "*** SWITCH TO TOP DOWN\n",
      "*** SWITCH TO BOTTOM UP\n",
      "*** SWITCH TO TOP DOWN\n",
      "*** SWITCH TO BOTTOM UP\n",
      "Nr edges in chart: 37\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not all parses found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-56d34d561717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'John saw a dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumparses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/linzhun/anaconda/lib/python3.6/site-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(choice, print_times, print_grammar, print_trees, trace, sent, numparses)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nr edges in chart:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumparses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnumparses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not all parses found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_trees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not all parses found"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.parse.chart.demo(5, print_times=False, trace=1,sent='John saw a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature chart parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grammar with 18 productions (start state = S[])\n",
      "    S[] -> NP[] VP[]\n",
      "    PP[] -> Prep[] NP[]\n",
      "    NP[] -> NP[] PP[]\n",
      "    VP[] -> VP[] PP[]\n",
      "    VP[] -> Verb[] NP[]\n",
      "    VP[] -> Verb[]\n",
      "    NP[] -> Det[pl=?x] Noun[pl=?x]\n",
      "    NP[] -> 'John'\n",
      "    NP[] -> 'I'\n",
      "    Det[] -> 'the'\n",
      "    Det[] -> 'my'\n",
      "    Det[-pl] -> 'a'\n",
      "    Noun[-pl] -> 'dog'\n",
      "    Noun[-pl] -> 'cookie'\n",
      "    Verb[] -> 'ate'\n",
      "    Verb[] -> 'saw'\n",
      "    Prep[] -> 'with'\n",
      "    Prep[] -> 'under'\n",
      "\n",
      "* FeatureChartParser\n",
      "Sentence: I saw a dog\n",
      "|. I .saw. a .dog.|\n",
      "|[---]   .   .   .| [0:1] 'I'\n",
      "|.   [---]   .   .| [1:2] 'saw'\n",
      "|.   .   [---]   .| [2:3] 'a'\n",
      "|.   .   .   [---]| [3:4] 'dog'\n",
      "|[---]   .   .   .| [0:1] NP[] -> 'I' *\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[] * VP[] {}\n",
      "|[--->   .   .   .| [0:1] NP[] -> NP[] * PP[] {}\n",
      "|.   [---]   .   .| [1:2] Verb[] -> 'saw' *\n",
      "|.   [--->   .   .| [1:2] VP[] -> Verb[] * NP[] {}\n",
      "|.   [---]   .   .| [1:2] VP[] -> Verb[] *\n",
      "|.   [--->   .   .| [1:2] VP[] -> VP[] * PP[] {}\n",
      "|[-------]   .   .| [0:2] S[] -> NP[] VP[] *\n",
      "|.   .   [---]   .| [2:3] Det[-pl] -> 'a' *\n",
      "|.   .   [--->   .| [2:3] NP[] -> Det[pl=?x] * Noun[pl=?x] {?x: False}\n",
      "|.   .   .   [---]| [3:4] Noun[-pl] -> 'dog' *\n",
      "|.   .   [-------]| [2:4] NP[] -> Det[-pl] Noun[-pl] *\n",
      "|.   .   [------->| [2:4] S[] -> NP[] * VP[] {}\n",
      "|.   .   [------->| [2:4] NP[] -> NP[] * PP[] {}\n",
      "|.   [-----------]| [1:4] VP[] -> Verb[] NP[] *\n",
      "|.   [----------->| [1:4] VP[] -> VP[] * PP[] {}\n",
      "|[===============]| [0:4] S[] -> NP[] VP[] *\n",
      "(S[]\n",
      "  (NP[] I)\n",
      "  (VP[] (Verb[] saw) (NP[] (Det[-pl] a) (Noun[-pl] dog))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.parse.featurechart.demo(print_times=False,print_grammar=True,\n",
    "                             parser=nltk.parse.featurechart.FeatureChartParser,sent='I saw a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
