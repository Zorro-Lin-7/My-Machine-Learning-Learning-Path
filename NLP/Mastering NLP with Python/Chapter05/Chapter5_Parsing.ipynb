{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treebank construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identi ers for  les can be obtained using fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> import nltk.corpus\n",
    "   >>> print(str(nltk.corpus.treebank).replace('\\\\\\\\','/'))\n",
    "   <BracketParseCorpusReader in 'C:/nltk_data/corpora/treebank/combined'>\n",
    "   >>> nltk.corpus.treebank.fileids()\n",
    ">>> from nltk.corpus import treebank\n",
    ">>> print(treebank.words('wsj_0007.mrg'))\n",
    "['McDermott', 'International', 'Inc.', 'said', '0', ...]\n",
    ">>> print(treebank.tagged_words('wsj_0007.mrg'))\n",
    "[('McDermott', 'NNP'), ('International', 'NNP'), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accessing the Penn Treebank Corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> from nltk.corpus import treebank\n",
    "   >>> print(treebank.parsed_sents('wsj_0007.mrg')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> from nltk.corpus import treebank_chunk\n",
    "   >>> treebank_chunk.chunked_sents()[1]\n",
    "   Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Vinken', 'NNP')]), ('is',\n",
    "   'VBZ'), Tree('NP', [('chairman', 'NN')]), ('of', 'IN'), Tree('NP',\n",
    "   [('Elsevier', 'NNP'), ('N.V.', 'NNP')]), (',', ','), Tree('NP',\n",
    "   [('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group',\n",
    "   'NN')]), ('.', '.')])\n",
    "   >>> treebank_chunk.chunked_sents()[1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> from nltk.corpus import treebank_chunk\n",
    "   >>> treebank_chunk.chunked_sents()[1].leaves()\n",
    "   [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman',\n",
    "   'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','),\n",
    "   ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group',\n",
    "   'NN'), ('.', '.')]\n",
    "   >>> treebank_chunk.chunked_sents()[1].pos()\n",
    "   [(('Mr.', 'NNP'), 'NP'), (('Vinken', 'NNP'), 'NP'), (('is', 'VBZ'),\n",
    "   'S'), (('chairman', 'NN'), 'NP'), (('of', 'IN'), 'S'), (('Elsevier',\n",
    "   'NNP'), 'NP'), (('N.V.', 'NNP'), 'NP'), ((',', ','), 'S'), (('the',\n",
    "   'DT'), 'NP'), (('Dutch', 'NNP'), 'NP'), (('publishing', 'VBG'), 'NP'),\n",
    "   (('group', 'NN'), 'NP'), (('.', '.'), 'S')]\n",
    "   >>> treebank_chunk.chunked_sents()[1].productions()\n",
    "   [S -> NP ('is', 'VBZ') NP ('of', 'IN') NP (',', ',') NP ('.', '.'),\n",
    "   NP -> ('Mr.', 'NNP') ('Vinken', 'NNP'), NP -> ('chairman', 'NN'), NP\n",
    "   -> ('Elsevier', 'NNP') ('N.V.', 'NNP'), NP -> ('the', 'DT') ('Dutch',\n",
    "   'NNP') ('publishing', 'VBG') ('group', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> nltk.corpus.treebank.tagged_words()\n",
    "   [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tags and frequency \n",
    "\n",
    "obtains a list of tags and the frequency of each tag in the Treebank corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> from nltk.probability import FreqDist\n",
    "   >>> from nltk.corpus import treebank\n",
    "   >>> fd = FreqDist()\n",
    "    >>> fd.items()\n",
    "dict_items([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accessing the Sinica Treebank Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子'], ...]\n",
      "(S\n",
      "  (NP (NP (N‧的 (Nhaa 我) (DE 的)) (Ncb 腦海)) (Ncda 中))\n",
      "  (Dd 頓時)\n",
      "  (DM 一片)\n",
      "  (VH11 空白))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import sinica_treebank\n",
    "print(sinica_treebank.sents())\n",
    "print(sinica_treebank.parsed_sents()[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting Context Free Grammar (CFG) rules from Treebank\n",
    "\n",
    "• A set of non terminal nodes (N)\n",
    "• A set of terminal nodes (T)\n",
    "• Start symbol (S)\n",
    "• A set of production rules (P) of the form:\n",
    "   \n",
    "CFG rules are of two types—Phrase structure rules and Sentence structure rules.\n",
    "\n",
    "    A Phrase Structure Rule can be de ned as follows A->a, where A and a consists of Terminals and Non terminals.\n",
    "    n Sentence level Construction of CFG, there are four structures:\n",
    "        • Declarative structure: Deals with declarative sentences (the subject is followed by a predicate).\n",
    "        • Imperative structure: Deals with imperative sentences, commands,\n",
    "            or suggestions (sentences begin with a verb phrase and do not include a subject).\n",
    "        • Yes-No structure: Deals with question-answering sentences. The answers to these questions are either yes \n",
    "        or no.\n",
    "        • Wh-question structure: Deals with question-answering sentences. Questions that begin following Wh words               (Who, What, How, When, Where, Why, and Which).\n",
    "        \n",
    "General CFG rules are summarized here:\n",
    "![](CFG_rules.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:the use of Context-free Grammar rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> from nltk import Nonterminal, nonterminals, Production, CFG\n",
    "   >>> nonterminal1 = Nonterminal('NP')\n",
    "   >>> nonterminal2 = Nonterminal('VP')\n",
    "   >>> nonterminal3 = Nonterminal('PP')\n",
    "   >>> nonterminal1.symbol()\n",
    "   'NP'\n",
    "   >>> nonterminal2.symbol()\n",
    "   'VP'\n",
    "   >>> nonterminal3.symbol()\n",
    "   'PP'\n",
    "   >>> nonterminal1==nonterminal2\n",
    "   False\n",
    "   >>> nonterminal2==nonterminal3\n",
    "   False\n",
    "   >>> nonterminal1==nonterminal3\n",
    "   False\n",
    "   >>> S, NP, VP, PP = nonterminals('S, NP, VP, PP')\n",
    "   >>> N, V, P, DT = nonterminals('N, V, P, DT')\n",
    "   >>> production1 = Production(S, [NP, VP])\n",
    ">>> production2 = Production(NP, [DT, NP])\n",
    "   >>> production3 = Production(VP, [V, NP,NP,PP])\n",
    "   >>> production1.lhs()\n",
    "   S\n",
    "   >>> production1.rhs()\n",
    "   (NP, VP)\n",
    "   >>> production3.lhs()\n",
    "   VP\n",
    "   >>> production3.rhs()\n",
    "   (V, NP, NP, PP)\n",
    "   >>> production3 == Production(VP, [V,NP,NP,PP])\n",
    "   True\n",
    "   >>> production2 == production3\n",
    "   False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example for accessing ATIS grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> gram1\n",
    "   <Grammar with 5517 productions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the testing sentences from ATIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> len(sent)\n",
    "   98\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> testingsent[1]\n",
    "   11\n",
    "   >>> testingsent[0]\n",
    "   ['list', 'those', 'flights', 'that', 'stop', 'over', 'in', 'salt',\n",
    "   'lake', 'city', '.']\n",
    "   >>> sent=testingsent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom-up parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "                             >>> sent=testingsent[0]\n",
    "   >>> parser1 = nltk.parse.BottomUpChartParser(gram1)\n",
    "   >>> chart1 = parser1.chart_parse(sent)\n",
    "   >>> print((chart1.num_edges()))\n",
    "   13454\n",
    "   >>> print((len(list(chart1.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom-up, Left Corner parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser2 = nltk.parse.BottomUpLeftCornerChartParser(gram1)\n",
    "   >>> chart2 = parser2.chart_parse(sent)\n",
    "   >>> print((chart2.num_edges()))\n",
    "   8781\n",
    "   >>> print((len(list(chart2.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Corner parsing with a Bottom-up  lter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser3 = nltk.parse.LeftCornerChartParser(gram1)\n",
    "   >>> chart3 = parser3.chart_parse(sent)\n",
    "   >>> print((chart3.num_edges()))\n",
    "   1280\n",
    "   >>> print((len(list(chart3.parses(gram1.start())))))\n",
    "   11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-down parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "    >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>>parser4 = nltk.parse.TopDownChartParser(gram1)\n",
    "   >>> chart4 = parser4.chart_parse(sent)\n",
    "   >>> print((chart4.num_edges()))\n",
    "   37763\n",
    "   >>> print((len(list(chart4.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Incremental Bottom-up parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser5 = nltk.parse.IncrementalBottomUpChartParser(gram1)\n",
    "   >>> chart5 = parser5.chart_parse(sent)\n",
    "   >>> print((chart5.num_edges()))\n",
    "   13454\n",
    "   >>> print((len(list(chart5.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Incremental Bottom-up, Left Corner parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser6 = nltk.parse.IncrementalBottomUpLeftCornerChartParser(gr\n",
    "   am1)\n",
    "   >>> chart6 = parser6.chart_parse(sent)\n",
    "   >>> print((chart6.num_edges()))\n",
    "   8781\n",
    "   >>> print((len(list(chart6.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Incremental Left Corner parsing with a Bottom-up  filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser7 = nltk.parse.IncrementalLeftCornerChartParser(gram1)\n",
    "   >>> chart7 = parser7.chart_parse(sent)\n",
    "   >>> print((chart7.num_edges()))\n",
    "   1280\n",
    "   >>> print((len(list(chart7.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Incremental Top-down parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "   >>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser8 = nltk.parse.IncrementalTopDownChartParser(gram1)\n",
    "   >>> chart8 = parser8.chart_parse(sent)\n",
    "   >>> print((chart8.num_edges()))\n",
    "   37763\n",
    "   >>> print((len(list(chart8.parses(gram1.start())))))\n",
    "   11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Earley parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> gram1 = nltk.data.load('grammars/large_grammars/atis.cfg')\n",
    "   >>> sent = nltk.data.load('grammars/large_grammars/atis_sentences.\n",
    "   txt')\n",
    "   >>> sent = nltk.parse.util.extract_test_sentences(sent)\n",
    "   >>> testingsent=sent[25]\n",
    "   >>> sent=testingsent[0]\n",
    "   >>> parser9 = nltk.parse.EarleyChartParser(gram1)\n",
    "   >>> chart9 = parser9.chart_parse(sent)\n",
    "   >>> print((chart9.num_edges()))\n",
    "   37763\n",
    "   >>> print((len(list(chart9.parses(gram1.start())))))\n",
    "   11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a probabilistic Context Free Grammar from CFG\n",
    "\n",
    "概率上下文无关文法（PCFG），CFG中的所有production rules 都附带概率。这些概率的总和是1。它生成与CFG相同的解析结构，但它也给每个解析树分配一个概率。通过生成树中所使用的所有生产规则的概率乘积，得到经过分析的树的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates the formation of rules in PCFG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> from nltk.corpus import treebank\n",
    "   >>> from itertools import islice\n",
    "   >>> from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "   >>> gram2 = PCFG.from string(\"\"\"\n",
    "   A -> B B [.3] | C B C [.7]\n",
    "   B -> B D [.5] | C [.5]\n",
    "   C -> 'a' [.1] | 'b' [0.9]\n",
    "   D -> 'b' [1.0]\n",
    "   \"\"\")\n",
    "   >>> prod1 = gram2.productions()[0]\n",
    "   >>> prod1\n",
    "   A -> B B [0.3]\n",
    "   >>> prod2 = gram2.productions()[1]\n",
    "   >>> prod2\n",
    "   A -> C B C [0.7]\n",
    "   >>> prod2.lhs()\n",
    "   A\n",
    "   >>> prod2.rhs()\n",
    "   (C, B, C)\n",
    "   >>> print((prod2.prob()))\n",
    "   0.7\n",
    "   >>> gram2.start()\n",
    "   A\n",
    "   >>> gram2.productions()\n",
    "   [A -> B B [0.3], A -> C B C [0.7], B -> B D [0.5], B -> C [0.5], C ->\n",
    "   'a' [0.1], C -> 'b' [0.9], D -> 'b' [1.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates Probabilistic Chart Parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> from nltk.corpus import treebank\n",
    "   >>> from itertools import islice\n",
    "   >>> from nltk.grammar import PCFG, induce_pcfg, toy_pcfg1, toy_pcfg2\n",
    "   >>> tokens = \"Jack told Bob to bring my cookie\".split()\n",
    "   >>> grammar = toy_pcfg2\n",
    "   >>> print(grammar)\n",
    "   Grammar with 23 productions (start state = S)\n",
    "       S -> NP VP [1.0]\n",
    "       VP -> V NP [0.59]\n",
    "       VP -> V [0.4]\n",
    "       VP -> VP PP [0.01]\n",
    "       NP -> Det N [0.41]\n",
    "       NP -> Name [0.28]\n",
    "       NP -> NP PP [0.31]\n",
    "       PP -> P NP [1.0]\n",
    "       V -> 'saw' [0.21]\n",
    "       V -> 'ate' [0.51]\n",
    "       V -> 'ran' [0.28]\n",
    "       N -> 'boy' [0.11]\n",
    "       N -> 'cookie' [0.12]\n",
    "       N -> 'table' [0.13]\n",
    "       N -> 'telescope' [0.14]\n",
    "       N -> 'hill' [0.5]\n",
    "       Name -> 'Jack' [0.52]\n",
    "       Name -> 'Bob' [0.48]\n",
    "       P -> 'with' [0.61]\n",
    "       P -> 'under' [0.39]\n",
    "       Det -> 'the' [0.41]\n",
    "       Det -> 'a' [0.31]\n",
    "       Det -> 'my' [0.28]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYK chart parsing algorithm\n",
    "\n",
    "递归下降分析的缺点是它导致了左递归问题，而且非常复杂。因此，引入CYK图解析。它使用the Dynamic Programming approach\n",
    "CYK算法可以在O（n3）时间里构建一个chart\n",
    "\n",
    "Both CYK and Earley are Bottom-up chart parsing algorithms. But, the Earley algorithm also makes use of Top-down predictions when invalid parses are constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example of CYK parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = [\"the\", \"kids\", \"opened\", \"the\", \"box\", \"on\", \"the\", \"floor\"]\n",
    "   gram = nltk.parse_cfg(\"\"\"\n",
    "   S -> NP VP\n",
    "   NP -> Det N | NP PP\n",
    "   VP -> V NP | VP PP\n",
    "   PP -> P NP\n",
    "   Det -> 'the'\n",
    "   N -> 'kids' | 'box' | 'floor'\n",
    "   V -> 'opened' P -> 'on'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earley chart parsing algorithm\n",
    "\n",
    "该算法是类似于自顶向下分析。它可以处理左递归，而且不需要CNF。它填充表格的方式是“从左到右”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Earley chart parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " >>> import nltk\n",
    "   >>> nltk.parse.earleychart.demo(print_times=False, trace=1,sent='I saw\n",
    "   a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Chart parser in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> nltk.parse.chart.demo(2, print_times=False, trace=1,sent='John saw\n",
    "   a dog', numparses=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrates parsing using the Stepping Chart parser in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    "   >>> nltk.parse.chart.demo(5, print_times=False, trace=1,sent='John saw\n",
    "   a dog', numparses=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature chart parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grammar with 18 productions (start state = S[])\n",
      "    S[] -> NP[] VP[]\n",
      "    PP[] -> Prep[] NP[]\n",
      "    NP[] -> NP[] PP[]\n",
      "    VP[] -> VP[] PP[]\n",
      "    VP[] -> Verb[] NP[]\n",
      "    VP[] -> Verb[]\n",
      "    NP[] -> Det[pl=?x] Noun[pl=?x]\n",
      "    NP[] -> 'John'\n",
      "    NP[] -> 'I'\n",
      "    Det[] -> 'the'\n",
      "    Det[] -> 'my'\n",
      "    Det[-pl] -> 'a'\n",
      "    Noun[-pl] -> 'dog'\n",
      "    Noun[-pl] -> 'cookie'\n",
      "    Verb[] -> 'ate'\n",
      "    Verb[] -> 'saw'\n",
      "    Prep[] -> 'with'\n",
      "    Prep[] -> 'under'\n",
      "\n",
      "* FeatureChartParser\n",
      "Sentence: I saw a dog\n",
      "|. I .saw. a .dog.|\n",
      "|[---]   .   .   .| [0:1] 'I'\n",
      "|.   [---]   .   .| [1:2] 'saw'\n",
      "|.   .   [---]   .| [2:3] 'a'\n",
      "|.   .   .   [---]| [3:4] 'dog'\n",
      "|[---]   .   .   .| [0:1] NP[] -> 'I' *\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[] * VP[] {}\n",
      "|[--->   .   .   .| [0:1] NP[] -> NP[] * PP[] {}\n",
      "|.   [---]   .   .| [1:2] Verb[] -> 'saw' *\n",
      "|.   [--->   .   .| [1:2] VP[] -> Verb[] * NP[] {}\n",
      "|.   [---]   .   .| [1:2] VP[] -> Verb[] *\n",
      "|.   [--->   .   .| [1:2] VP[] -> VP[] * PP[] {}\n",
      "|[-------]   .   .| [0:2] S[] -> NP[] VP[] *\n",
      "|.   .   [---]   .| [2:3] Det[-pl] -> 'a' *\n",
      "|.   .   [--->   .| [2:3] NP[] -> Det[pl=?x] * Noun[pl=?x] {?x: False}\n",
      "|.   .   .   [---]| [3:4] Noun[-pl] -> 'dog' *\n",
      "|.   .   [-------]| [2:4] NP[] -> Det[-pl] Noun[-pl] *\n",
      "|.   .   [------->| [2:4] S[] -> NP[] * VP[] {}\n",
      "|.   .   [------->| [2:4] NP[] -> NP[] * PP[] {}\n",
      "|.   [-----------]| [1:4] VP[] -> Verb[] NP[] *\n",
      "|.   [----------->| [1:4] VP[] -> VP[] * PP[] {}\n",
      "|[===============]| [0:4] S[] -> NP[] VP[] *\n",
      "(S[]\n",
      "  (NP[] I)\n",
      "  (VP[] (Verb[] saw) (NP[] (Det[-pl] a) (Noun[-pl] dog))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.parse.featurechart.demo(print_times=False,print_grammar=True,\n",
    "                             parser=nltk.parse.featurechart.FeatureChartParser,sent='I saw a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
