{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text1 = nltk.word_tokenize(\"It is a pleasant day today\")\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penn Treebank (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# information about the NNS tag:\n",
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "# a regular expression may also be queried:The code gives information regarding all the tags of verb phrases.\n",
    "nltk.help.upenn_tagset('VB.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个通过词性标注实现词义消歧的例子。bear是一个动词，容忍，它也是一种动物，名词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('bear', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('pain', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('bear', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"I cannot bear the pain of bear\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a tagged token is represented as a tuple consisting of a token and its tag.\n",
    "\n",
    "We can create this tuple in NLTK using the str2tuple() function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bear', 'NN')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedword = nltk.tag.str2tuple('bear/NN')\n",
    "taggedword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bear'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedword[0]\n",
    "taggedword[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从给定文本生成元组序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('sacred', 'VBN'),\n",
       " ('Ganga', 'NNP'),\n",
       " ('flows', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('region', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pilgrimage', 'NN'),\n",
       " ('.', '.'),\n",
       " ('People', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"The/DT sacred/VBN Ganga/NNP flows/VBZ in/IN this/DT\n",
    "   region/NN ./. This/DT is/VBZ a/DT pilgrimage/NN ./. People/NNP from/IN\n",
    "   all/DT over/IN the/DT country/NN visit/NN this/DT place/NN ./. \"\"\"\n",
    "\n",
    "[nltk.tag.str2tuple(t) for t in sentence.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将元组（单词,词性标记）转换为单词和标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bear/NN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedtok = ('bear', 'NN')\n",
    "from nltk.tag.util import tuple2str\n",
    "tuple2str(taggedtok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the occurence of some common tags in the Treebank corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tag = nltk.FreDist(tag for (word, tag) in treebank_tagged)\n",
    "tag.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculates the number of tags occurring before a noun tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " '.',\n",
       " 'VERB',\n",
       " 'NUM',\n",
       " 'PRT',\n",
       " 'CONJ',\n",
       " 'PRON',\n",
       " 'X',\n",
       " 'ADV']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tagpairs = nltk.bigrams(treebank_tagged)\n",
    "preceders_noun = [x[1] for (x,y) in tagpairs if y[1] == 'NOUN']\n",
    "freqdist = nltk.FreqDist(preceders_noun)\n",
    "[tag for (tag, _) in freqdist.most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "illustrates the creation of a tuple (word:pos tag) using dictionaries in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 'ADJ', 'boy': 'N', 'generously': 'ADV', 'read': 'V'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = {}\n",
    "tag['beautiful'] = 'ADJ'\n",
    "tag['boy'] = 'N'\n",
    "tag['read'] = 'V'\n",
    "tag['generously'] = 'ADV'\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default tagging 将相同的POS tags 分配给所有tokens。\n",
    "\n",
    "SequentialBackoffTagger 的子类 是 DefaultTagger；\n",
    "\n",
    "![hierarchy of tagger](DefaultTagger.png)\n",
    "\n",
    "choose_tag() method 必须由SequentialBackoffTagger 实现，其参数包括：\n",
    "\n",
    "    • A collection of tokens\n",
    "    • The index of the token that should be tagged\n",
    "    • The previous tags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beautiful', 'NN'), ('morning', 'NN'), ('good', 'NN'), ('Do', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger('NN')\n",
    "tag.tag(['Beautiful', 'morning','good','Do']) # 将相同的POS tags 分配给所有tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert a tagged sentence into an untagged sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful', 'morning']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag([('beautiful', 'NN'), ('morning', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating POS-tagged corpora\n",
    "A corpus may be known as a collection of documents. A corpora is the collection of multiple corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a data directory named ~/nltkdoc inside the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os,os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create = os.path.expanduser('~/nltkdoc') # 在主页目录下创建一个名为nltkdoc的目录\n",
    "if not os.path.exists(create):\n",
    "    os.mkdir(create)\n",
    "\n",
    "os.path.exists(create) # 验证nltkdoc 目录是否存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_data = os.path.expanduser('~/nltk_data/data')\n",
    "os.mkdir(create_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names corpus:It consists of two  les, namely, male.txt and female.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2943"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "names.fileids()\n",
    "len(names.words('male.txt'))\n",
    "len(names.words('female.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a large collection of English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'en-basic']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "235886"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "words.fileids()\n",
    "len(words.words('en'))\n",
    "len(words.words('en-basic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging(词性标注)也被称为词类消歧或语法标记。\n",
    "\n",
    "分2类：\n",
    "* rule-based ，比如 **E. Brill's tagger**\n",
    "* stochastic/probabilistic.\n",
    "\n",
    "POS分类器以a document作为输入，得到词汇特征。它利用这些词汇特征和已有的train labels来训练。这种类型的分类器被称为二级分类器(a second order classier)，利用Bootstrap分类器以生成tags。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一个backoff分类器，会执行一个后退的程序。其output 以这样的方式获得：trigram POS tagger 依赖 bigram POS tagger，后者一类unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 训练一个POS classifier ，会生成一个特征集。特征集由以下组成：\n",
    "    * 现在的word\n",
    "    * 上一个word 或前缀\n",
    "    * 下个word 或承接的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最大熵分类器：训练时要找的最优参数 令corpus的总似然最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UnigramTagger training：仅采用一个token找到 其词性。初始化时，提供句子列表来训练UnigramTagger："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "training = treebank.tagged_sents()[:7000] # 用7000个句子来训练\n",
    "unitagger = UnigramTagger(training)\n",
    "treebank.sents()[0]\n",
    "unitagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate UnigramTagger， calculates the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619024159944167"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "unitagger = UnigramTagger(training)\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "unitagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个类的继承关系：\n",
    "![](Tagger_inheritance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since UnigramTagger inherits from ContextTagger，we can map the context key with a specic tag.\n",
    "\n",
    "ContextTagger标注器 用给定tag 的频率来决定最有可能的tag的标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', None),\n",
       " ('Vinken', 'NN'),\n",
       " (',', None),\n",
       " ('61', None),\n",
       " ('years', None),\n",
       " ('old', None),\n",
       " (',', None),\n",
       " ('will', None),\n",
       " ('join', None),\n",
       " ('the', None),\n",
       " ('board', None),\n",
       " ('as', None),\n",
       " ('a', None),\n",
       " ('nonexecutive', None),\n",
       " ('director', None),\n",
       " ('Nov.', None),\n",
       " ('29', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "unitag = UnigramTagger(model={'Vinken': 'NN'}) # only tags 'Vinken' with the 'NN'\n",
    "unitag.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluates UnigramTagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7972986842375351"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitagger = UnigramTagger(training, cutoff=5)\n",
    "unitagger.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backoff tagging ：\n",
    "如果一个tagger不能tag 一个token，那么这个token可能传给下一个tagger 来tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DefaultTagger: tag=NN>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9619024159944167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backoff:采用DefaultTagger and UnigramTagger \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "tag1 = DefaultTagger('NN')\n",
    "tag2 = UnigramTagger(training, backoff=tag1)\n",
    "tag2.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NgramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9171131227292321"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BigramTagger:\n",
    "import nltk\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.corpus import treebank\n",
    "training_1 = treebank.tagged_sents()[:7000]\n",
    "bigramtagger = BigramTagger(training_1)\n",
    "treebank.sents()[0]\n",
    "bigramtagger.tag(treebank.sents()[0])\n",
    "testing_1 = treebank.tagged_sents()[2000:]\n",
    "bigramtagger.evaluate(testing_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171131227292321"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9022107272615308"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BigramTagger and TrigramTagger:\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "bigramtag = BigramTagger(training)\n",
    "bigramtag.evaluate(testing)\n",
    "trigramtag = TrigramTagger(training)\n",
    "trigramtag.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NgramTagger, n > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9304554878173943"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk import NgramTagger\n",
    "\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "\n",
    "quadgramtag = NgramTagger(4, training)\n",
    "quadgramtag.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AffixTagger 也是一个ContextTagger（上下文依赖分类器），使用前缀和后缀作为上下文信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2902682841718497"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.tag import AffixTagger\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "affixtag = AffixTagger(training)\n",
    "affixtag.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learns the use of four character prefixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2094751318841472"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "prefixtag = AffixTagger(training, affix_length=4)\n",
    "prefixtag.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learns the use of three character suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2902682841718497"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "suffixtag = AffixTagger(training, affix_length=-3)\n",
    "suffixtag.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combines many affix taggers in the back-off chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2094751318841472"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.25841082168442225"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.2940451998275756"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.33072644046226163"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "prefixtagger = AffixTagger(training, affix_length=4)\n",
    "prefixtagger.evaluate(testing)\n",
    "\n",
    "prefixtagger3 = AffixTagger(training, affix_length=3, backoff=prefixtagger)\n",
    "prefixtagger3.evaluate(testing)\n",
    "\n",
    "suffixtagger3 = AffixTagger(training, affix_length=-3, backoff=prefixtagger3)\n",
    "suffixtagger3.evaluate(testing)\n",
    "\n",
    "suffixtagger4 = AffixTagger(training, affix_length=-4, backoff=suffixtagger3)\n",
    "suffixtagger4.evaluate(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TnT is Trigrams n Tags. TnT 一种基于统计的tagger，基于二阶马尔可夫模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882176652913768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import tnt\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing =treebank.tagged_sents()[2000:]\n",
    "\n",
    "tnt_tagger = tnt.TnT()\n",
    "tnt_tagger.train(training)\n",
    "tnt_tagger.evaluate(testing) # 训练时间较长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TnT computes ConditionalFreqDist and internalFreqDist from the training text.\n",
    "\n",
    " In order to choose the best tag, TnT uses the ngram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882176652913768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import tnt\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training = treebank.tagged_sents()[:7000]\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "\n",
    "tnt_tagger = tnt.TnT()\n",
    "unknown = DefaultTagger('NN')\n",
    "tagger_tnt = tnt.TnT(unk=unknown, Trained=True)#if the value of the unknown tagger is provided explicitly, \n",
    "                                                # then TRAINED will be set to TRUE\n",
    "tnt_tagger.train(training)\n",
    "tnt_tagger.evaluate(testing)   # 耗时相对较久"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a chunker using pos-tagged corpora\n",
    "\n",
    "Chunking 是一个过程，用以进行实体检测。用于分割并标记段落，每段多个句子，每个句子含tokens。\n",
    "\n",
    "要设计一个chunker，需要定义一个chunker grammer，它含有该如何chunking的规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT wise/JJ small/JJ girl/NN of/IN)\n",
      "  village/N\n",
      "  became/VBD\n",
      "  (NP leader/NN))\n"
     ]
    }
   ],
   "source": [
    "#Noun Phrase Chunking by forming the chunk rules:\n",
    "sent = [(\"A\",\"DT\"),(\"wise\", \"JJ\"), (\"small\", \"JJ\"),(\"girl\", \"NN\"),\n",
    "   (\"of\", \"IN\"), (\"village\", \"N\"),  (\"became\", \"VBD\"), (\"leader\", \"NN\")]\n",
    "grammer = \"NP: {<DT>?<JJ>*<NN><IN>?<NN>*}\"\n",
    "find = nltk.RegexpParser(grammer)\n",
    "res = find.parse(sent)\n",
    "print(res)\n",
    "# res.draw()  # 另开窗口作图，不适合Jupyter notebok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](parse_tree.png)\n",
    "**DT** as optional, any number of **JJ**, followed by **NN**, optional **IN**, and any number of **NN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP financial/NN year/NN account/NN summary/NN))\n"
     ]
    }
   ],
   "source": [
    "#Noun Phrase chunk rule is created with any number of nouns:\n",
    "noun1 = [('financial', 'NN'), ('year', 'NN'), ('account', 'NN'), ('summary', 'NN')]\n",
    "gram = 'NP:{<NN>+}'\n",
    "find = nltk.RegexpParser(gram)\n",
    "print(find.parse(noun1))\n",
    "\n",
    "#x = find.parse(noun1)\n",
    "#x.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](parse_tree2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用整个chunk，也可使用chunk的中间部分，其余部分淘汰；或者块的一部分可以从块的开始或从块的结尾使用，并且块的剩余部分被移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UnigramChunker 源码："
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
