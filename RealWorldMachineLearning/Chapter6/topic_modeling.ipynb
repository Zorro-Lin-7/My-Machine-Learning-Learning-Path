{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主题建模的工作原理是，识别一个文件中的主题关键词。这些词将决定文件内容的主题是关于什么的。\n",
    "\n",
    "1、我们采用的是正则表达式标注器 RegexpTokenizer，因为我们只想得到words,而不需要标点符号或其他tokens；\n",
    "\n",
    "2、移除停用词是另一个重要步骤，这有助于我们排除噪声，如is,the这次词；\n",
    "\n",
    "3、之后，我们需要截取词干，获得词的原型。\n",
    "\n",
    "以上3个步骤我们打包成了一个preprocessor类。\n",
    "\n",
    "用于建模的技法是Latent Dirichlet Allocation (LDA) 隐含狄利克雷分布。LDA基本上是把一篇文档看作一组主题的集合，即一篇文档可以有多个主题。LDA可以将每篇文档的主题按照概率分布的形式给出。\n",
    "\n",
    "文档中的words,每个词都由其中某一主题生成，所有的词组成了一篇文档。各主题依据一定概率生成对应的词，我们现拥有词，目的是找出topics。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load input file\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To define a class to preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    # Initialize various operators\n",
    "    def __init__(self):\n",
    "        # Create a regular expression tokenizer\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        \n",
    "        # 停用词列表，取自nltk的语料库。分析时排除它们，如一些常用词is,in,the\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "        \n",
    "        # Create a stemmer\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        \n",
    "        # Define a processor function(method) that takes care of \n",
    "        # Tokenization, stop words removal, stemming\n",
    "    def process(self, input_text):\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower()) # tokenize the string\n",
    "            \n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english] # remove the stop words\n",
    "            \n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords] # Perform stemming on the tokens\n",
    "            \n",
    "        return tokens_stemmed\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = 'data_topic_modeling.txt'\n",
    "data = load_data(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He spent a lot of time studying cryptography. ',\n",
       " 'You need to have a very good understanding of modern encryption systems in order to work there.',\n",
       " \"If their team doesn't win this match, they will be out of the competition.\",\n",
       " 'Those codes are generated by a specialized machine. ',\n",
       " 'The club needs to develop a policy of training and promoting younger talent. ',\n",
       " 'His movement off the ball is really great. ',\n",
       " 'In order to evade the defenders, he needs to move swiftly.',\n",
       " 'We need to make sure only the authorized parties can read the message.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()  # create a preprocessor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tokens = [preprocessor.process(x) for x in data] # Create a list for processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['spent', 'lot', 'time', 'studi', 'cryptographi'],\n",
       " ['need',\n",
       "  'good',\n",
       "  'understand',\n",
       "  'modern',\n",
       "  'encrypt',\n",
       "  'system',\n",
       "  'order',\n",
       "  'work'],\n",
       " ['team', 'win', 'match', 'competit'],\n",
       " ['code', 'generat', 'special', 'machin'],\n",
       " ['club', 'need', 'develop', 'polici', 'train', 'promot', 'younger', 'talent'],\n",
       " ['movement', 'ball', 'realli', 'great'],\n",
       " ['order', 'evad', 'defend', 'need', 'move', 'swift'],\n",
       " ['need', 'make', 'sure', 'author', 'parti', 'read', 'messag']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将tokens（分词后）转换为字典格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x11441f828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tokens = corpora.Dictionary(processed_tokens) # Create a dictionary based on the tokenized documents 用于主题建模\n",
    "dict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (14, 1), (15, 1), (16, 1)],\n",
       " [(17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(5, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)],\n",
       " [(28, 1), (29, 1), (30, 1), (31, 1)],\n",
       " [(5, 1), (11, 1), (32, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(5, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建 document-term matrix 文档-词矩阵\n",
    "corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采用隐含狄利克雷分布 Latent Dirichlet Allocation(LDA) 来做主题建模\n",
    "\n",
    "隐含狄利克雷分布简称LDA(Latent Dirichlet allocation)，是一种主题模型，它可以将每篇文档的主题按照概率分布的形式给出。\n",
    "\n",
    "同时它是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the LDA model based on the corpus we just created\n",
    "# 先定义所需的参数，并初始化 LDA model object\n",
    "\n",
    "num_topics = 2  # 假设文本可以被分成2个主题\n",
    "num_words = 4\n",
    "\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dict_tokens, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.050*\"studi\" + 0.050*\"lot\" + 0.050*\"time\" + 0.050*\"cryptographi\"'),\n",
       " (1, '0.078*\"need\" + 0.043*\"order\" + 0.026*\"work\" + 0.026*\"good\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=num_topics,num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判别出2个主题后，通过most-contributed words ,我们能看出它是如何划分这2个主题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributed words to the topics:\n",
      "\n",
      "Topic 0 ==> 0.050*\"studi\" + 0.050*\"lot\" + 0.050*\"time\" + 0.050*\"cryptographi\"\n",
      "\n",
      "Topic 1 ==> 0.078*\"need\" + 0.043*\"order\" + 0.026*\"work\" + 0.026*\"good\"\n"
     ]
    }
   ],
   "source": [
    "print('Most contributed words to the topics:')\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "    print('\\nTopic',item[0],'==>',item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
