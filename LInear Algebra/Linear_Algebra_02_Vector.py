# 来源：http://www.hahack.com/math/math-vector/

'''
向量是机器学习中的基础数据表示形式，例如计算机阅读文本的过程首先会将文本分词，然后用向量表示。这是因为
向量很适合在高维空间中表达和处理。机器学习中，诸如投影、降维的概念，都是在向量的基础上做的。
Numpy 中，直接用ndarray 表示向量。
'''
import numpy as np

# 加减
a = np.array([-1, 2])
b = np.array([3, 1])
a + b
a - b  # 几何上，减向量相当于加上一个反向的向量
-------------
# 乘法
 # 标量 x 向量
print(a * 3)
---------
# 向量点积（向量 x 向量）= 标量
product = a.dot(b) # 注：不是 * 运算
product = np.dot(a,b)

 # 向量长度
a_len = np.sqrt(a.dot(a))

 # 柯西不等式：对2个非0向量x,y, 有 |xy|<=|x||y|; 当且仅当x=cy 时取等号。 非常重要。
 ''' 从几何的角度来说，向量的点积与向量间夹角θ的余弦有关：ab = |a||b|cosθ
     点积反映了向量a在b上的投影，即两个向量在同一方向上的相同程度。
     当2个向量正交时，cosθ = 0，点积=0，投影最小；
     当2个向量平行时，cosθ = 1，点积最大，投影也最大
 '''
---------
#向量外积：列向量x行向量 称作向量的外积
'''
向量外积，又叫叉乘、叉积、向量积。得到的是向量，且垂直与2个原向量组成的平面，即法向量。
一个作用是得到一个和a, b 两个原向量正交的向量c。
几何意义，外积与sinθy有关：aXb = |a||b|sinθ
意味着，外积反映了a与b的正交程度。
当2个向量平行时，sinθ = 0，外积 = 0，正交程度最小；
当2个向量正交时，sinθ = 1, 外积最大，正交程度最大
'''
a = np.array([3, 5, 2])
b = np.array([1, 4, 7])
print(np.cross(a, b))

-----------
# 矩阵向量积：矩阵x向量
'''
矩阵的所有列向量的线性组合，向量的每个分量是每列的权值。
√ 一个矩阵其实就是一个线性变换。一个矩阵乘以一个向量后得到的向量，相当于将这个向量进行了线性变换。
'''

a = np.matrix('4 3 1;1 2 5')
x = np.array([[5], [2], [7]])
print(a*x) # 可以直接*

------------------
# 向量的转置

a = np.array([[2, 4]])  # 注意，用了2个[]以生成二维向量
a.T
b = np.array([2, 4]) # 一维的转置不会变化
b.T 

------
# 线性无关
'''
一组向量的张成空间，指这些向量随便线性组合后，能够表示多少个向量。记为span(a,b)。

当一个向量集合里的每个向量都对张成的空间有贡献，没有多余向量时，称这个向量集合线性无关。
换句话说，存在一个向量，可由其他向量线性组合得到，该向量即多余。反之线性相关。
例如，a = [1;2],b=[3,4],c = 2a =[2;4]。{a,b}张成的空间与{a,b,c}张成的空间一样，c是多余的，a、c线性相关。

能够表示一个空间的最少向量组合，成为空间的基。
'''

